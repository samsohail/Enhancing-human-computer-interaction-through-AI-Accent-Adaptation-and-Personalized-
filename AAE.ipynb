{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7mMKTH3o9VJ",
        "outputId": "b0826a02-0c0d-4f78-f439-d60e6dfd2609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchaudio\n",
        "!pip install transformers\n",
        "!pip install jiwer\n",
        "!pip install audiomentations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKTLqTGOpbx1",
        "outputId": "516e9b67-b852-48d6-8a12-ca483dd2ff14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: torch==2.4.0 in /usr/local/lib/python3.10/dist-packages (from torchaudio) (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchaudio) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchaudio) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchaudio) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchaudio) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchaudio) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchaudio) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0->torchaudio) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0->torchaudio) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-3.0.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.7)\n",
            "Collecting rapidfuzz<4,>=3 (from jiwer)\n",
            "  Downloading rapidfuzz-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading jiwer-3.0.4-py3-none-any.whl (21 kB)\n",
            "Downloading rapidfuzz-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-3.0.4 rapidfuzz-3.9.7\n",
            "Collecting audiomentations\n",
            "  Downloading audiomentations-0.37.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from audiomentations) (1.26.4)\n",
            "Collecting numpy-minmax<1,>=0.3.0 (from audiomentations)\n",
            "  Downloading numpy_minmax-0.3.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting numpy-rms<1,>=0.4.2 (from audiomentations)\n",
            "  Downloading numpy_rms-0.4.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: librosa!=0.10.0,<0.11.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from audiomentations) (0.10.2.post1)\n",
            "Collecting scipy<1.13,>=1.4 (from audiomentations)\n",
            "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soxr<1.0.0,>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from audiomentations) (0.5.0.post1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.3.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.8.2)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.0.8)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from numpy-minmax<1,>=0.3.0->audiomentations) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.0->numpy-minmax<1,>=0.3.0->audiomentations) (2.22)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (24.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (4.3.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2024.8.30)\n",
            "Downloading audiomentations-0.37.0-py3-none-any.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.5/80.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy_minmax-0.3.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading numpy_rms-0.4.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17 kB)\n",
            "Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy, numpy-rms, numpy-minmax, audiomentations\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "osqp 0.6.7.post0 requires scipy!=1.12.0,>=0.13.2, but you have scipy 1.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed audiomentations-0.37.0 numpy-minmax-0.3.1 numpy-rms-0.4.2 scipy-1.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "from torch.nn import functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import librosa\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import write\n",
        "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n",
        "from jiwer import wer\n",
        "\n",
        "# Step 1: Define Data Augmentation Pipeline\n",
        "augment = Compose([\n",
        "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
        "    TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n",
        "    PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
        "    Shift(min_shift=-0.5, max_shift=0.5, p=0.5)\n",
        "])\n",
        "\n",
        "# Custom Dataset for Loading Audio Files with Augmentation\n",
        "class SpeechDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None, target_length=80000, num_files=100, apply_augmentation=False):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.target_length = target_length\n",
        "        self.apply_augmentation = apply_augmentation\n",
        "        self.audio_files = [f for f in os.listdir(data_dir) if f.endswith('.wav')][:num_files]\n",
        "\n",
        "        if len(self.audio_files) == 0:\n",
        "            raise ValueError(f\"No audio files found in directory: {data_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        wav_path = os.path.join(self.data_dir, self.audio_files[idx])\n",
        "        if not os.path.exists(wav_path):\n",
        "            raise FileNotFoundError(f\"Audio file not found: {wav_path}\")\n",
        "\n",
        "        try:\n",
        "            waveform, sr = torchaudio.load(wav_path)\n",
        "        except RuntimeError as e:\n",
        "            print(f\"Error loading audio file: {wav_path}, Error: {e}\")\n",
        "            raise e\n",
        "\n",
        "        # Pad or truncate to the target length\n",
        "        if waveform.shape[1] < self.target_length:\n",
        "            padding = self.target_length - waveform.shape[1]\n",
        "            waveform = F.pad(waveform, (0, padding))\n",
        "        else:\n",
        "            waveform = waveform[:, :self.target_length]\n",
        "\n",
        "        # Apply Augmentation\n",
        "        if self.apply_augmentation:\n",
        "            waveform = augment_audio(waveform)\n",
        "\n",
        "        # Apply normalization if any\n",
        "        if self.transform:\n",
        "            waveform = self.transform(waveform)\n",
        "\n",
        "        return waveform, self.audio_files[idx]\n",
        "\n",
        "# Function to Apply Augmentation to the Audio\n",
        "def augment_audio(audio):\n",
        "    augmented_samples = augment(samples=audio.numpy(), sample_rate=16000)\n",
        "    return torch.tensor(augmented_samples)\n",
        "\n",
        "# Define Normalization Transform\n",
        "def normalize_waveform(waveform):\n",
        "    return (waveform - waveform.mean()) / waveform.std()\n",
        "\n",
        "# Directories for Data\n",
        "data_dir_A = \"/content/drive/MyDrive/data/extracted_files-3/en/North_American_English_W/\"\n",
        "\n",
        "# Target length for all audio files\n",
        "target_length = 80000\n",
        "\n",
        "# Initialize Dataset with 550 Files and Apply Augmentation\n",
        "dataset_A = SpeechDataset(data_dir_A, transform=normalize_waveform, target_length=target_length, num_files=100, apply_augmentation=True)\n",
        "\n",
        "# Initialize DataLoader\n",
        "dataloader_A = DataLoader(dataset_A, batch_size=1, shuffle=True)\n",
        "\n",
        "# Define AAE Components: Encoder, Decoder, Discriminator, Transformer-based Encoder\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, input_dim=80000, d_model=512, nhead=8, num_layers=6):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.fc_in = nn.Linear(input_dim, d_model)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead), num_layers=num_layers\n",
        "        )\n",
        "        self.fc_out = nn.Linear(d_model, 64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Flatten the input to (batch_size, seq_length)\n",
        "        x = self.fc_in(x)  # Linear transformation to match d_model\n",
        "        x = x.unsqueeze(1)  # Add sequence length dimension for Transformer\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.mean(dim=1)  # Global average pooling over the sequence length\n",
        "        return self.fc_out(x)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 80000)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.main(x)\n",
        "        x = x.view(x.size(0), 1, -1)\n",
        "        return x\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(64, 32),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.main(x)\n",
        "        return x\n",
        "\n",
        "# Initialize Models\n",
        "encoder = TransformerEncoder()\n",
        "decoder = Decoder()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# Initialize Loss Functions and Optimizers\n",
        "criterion_reconstruction = nn.MSELoss()\n",
        "criterion_adversarial = nn.BCELoss()\n",
        "optimizer_enc_dec = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.0002)\n",
        "optimizer_disc = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
        "\n",
        "# Set device to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "encoder.to(device)\n",
        "decoder.to(device)\n",
        "discriminator.to(device)\n",
        "\n",
        "# Function to Compute MCD (Placeholder)\n",
        "def compute_mcd(orig_audio, conv_audio):\n",
        "    return np.random.random()\n",
        "\n",
        "# Evaluation function for Mean Mel-Cepstral Distortion (MCD)\n",
        "def evaluate_metrics(original_dir, converted_dir, mapping):\n",
        "    mcd_scores = []\n",
        "\n",
        "    for orig_file, conv_file in mapping.items():\n",
        "        orig_path = os.path.join(original_dir, orig_file)\n",
        "        conv_path = os.path.join(converted_dir, conv_file)\n",
        "\n",
        "        if not os.path.exists(orig_path):\n",
        "            print(f\"Original file not found: {orig_path}\")\n",
        "            continue\n",
        "        if not os.path.exists(conv_path):\n",
        "            print(f\"Converted file not found: {conv_path}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            orig_audio, _ = torchaudio.load(orig_path)\n",
        "            conv_audio, _ = torchaudio.load(conv_path)\n",
        "        except RuntimeError as e:\n",
        "            print(f\"Error loading audio files. Original: {orig_path}, Converted: {conv_path}, Error: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Compute MCD\n",
        "        mcd_score = compute_mcd(orig_audio, conv_audio)\n",
        "        mcd_scores.append(mcd_score)\n",
        "\n",
        "    if mcd_scores:\n",
        "        mean_mcd = np.mean(mcd_scores)\n",
        "        print(f\"Mean MCD: {mean_mcd:.4f}\")\n",
        "    else:\n",
        "        print(\"No valid scores computed due to missing or corrupt files.\")\n",
        "\n",
        "# Function to save audio data correctly\n",
        "def save_audio(file_path, audio_tensor, sample_rate=16000):\n",
        "    audio_np = audio_tensor.cpu().detach().numpy().squeeze(0)\n",
        "    audio_np = audio_np / np.max(np.abs(audio_np) + 1e-6)\n",
        "    audio_np = np.clip(audio_np, -1, 1)\n",
        "    audio_np = (audio_np * 32767).astype(np.int16)\n",
        "\n",
        "    if len(audio_np.shape) > 1:\n",
        "        audio_np = audio_np[0]\n",
        "\n",
        "    write(file_path, sample_rate, audio_np)\n",
        "\n",
        "# Training Loop for Transformer-based AAE\n",
        "def train_AAE(dataloader_A, num_epochs=5):\n",
        "    filename_mapping_A = {}  # Mapping for evaluation\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        batch_count = 0\n",
        "        for real_A, file_A in dataloader_A:\n",
        "            real_A = real_A.to(device)\n",
        "            batch_count += 1\n",
        "\n",
        "            # Forward pass through Encoder and Decoder\n",
        "            latent = encoder(real_A)\n",
        "            reconstructed_A = decoder(latent)\n",
        "\n",
        "            # Compute Reconstruction Loss\n",
        "            loss_reconstruction = criterion_reconstruction(reconstructed_A, real_A)\n",
        "\n",
        "            # Adversarial Loss\n",
        "            optimizer_disc.zero_grad()\n",
        "            true_labels = torch.ones(latent.size(0), 1).to(device)\n",
        "            fake_labels = torch.zeros(latent.size(0), 1).to(device)\n",
        "\n",
        "            # Train Discriminator\n",
        "            loss_disc_real = criterion_adversarial(discriminator(latent.detach()), true_labels)\n",
        "            fake_latent = torch.randn_like(latent).to(device)\n",
        "            loss_disc_fake = criterion_adversarial(discriminator(fake_latent), fake_labels)\n",
        "            loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
        "            loss_disc.backward()\n",
        "            optimizer_disc.step()\n",
        "\n",
        "            # Train Encoder and Decoder with Adversarial Loss\n",
        "            optimizer_enc_dec.zero_grad()\n",
        "            loss_adv = criterion_adversarial(discriminator(latent), true_labels)\n",
        "            loss_enc_dec = loss_reconstruction + loss_adv\n",
        "            loss_enc_dec.backward()\n",
        "            optimizer_enc_dec.step()\n",
        "\n",
        "            # Save Converted Audio for Evaluation\n",
        "            converted_A_path = f\"/content/drive/MyDrive/data/extracted_files-3/en/aae_converted_epoch_{epoch}/{file_A[0]}\"\n",
        "            os.makedirs(os.path.dirname(converted_A_path), exist_ok=True)\n",
        "\n",
        "            save_audio(converted_A_path, reconstructed_A, sample_rate=16000)\n",
        "\n",
        "            # Update Mappings for Evaluation\n",
        "            filename_mapping_A[file_A[0]] = file_A[0]  # Map original to new\n",
        "\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_count}], Loss D: {loss_disc.item():.4f}, Loss Enc-Dec: {loss_enc_dec.item():.4f}\")\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] completed.\")\n",
        "        evaluate_aae(epoch, filename_mapping_A)\n",
        "\n",
        "    print(\"Training completed successfully!\")\n",
        "\n",
        "# Evaluation function for AAE\n",
        "def evaluate_aae(epoch, mapping_A):\n",
        "    original_dir_A = \"/content/drive/MyDrive/data/extracted_files-3/en/North_American_English_W/\"\n",
        "    converted_dir_A = f\"/content/drive/MyDrive/data/extracted_files-3/en/aae_converted_epoch_{epoch}\"\n",
        "\n",
        "    print(f\"Evaluating AAE performance after Epoch {epoch+1}\")\n",
        "    evaluate_metrics(original_dir=original_dir_A, converted_dir=converted_dir_A, mapping=mapping_A)\n",
        "\n",
        "# Start Training for AAE\n",
        "train_AAE(dataloader_A)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqTAT6TEpOqn",
        "outputId": "8c41e5e8-40e4-485e-dda3-f47143869e11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Batch [1], Loss D: 0.6609, Loss Enc-Dec: 1.6031\n",
            "Epoch [1/5], Batch [2], Loss D: 0.6608, Loss Enc-Dec: 1.6353\n",
            "Epoch [1/5], Batch [3], Loss D: 0.6218, Loss Enc-Dec: 1.3325\n",
            "Epoch [1/5], Batch [4], Loss D: 0.4881, Loss Enc-Dec: 1.2348\n",
            "Epoch [1/5], Batch [5], Loss D: 0.5682, Loss Enc-Dec: 1.4199\n",
            "Epoch [1/5], Batch [6], Loss D: 0.3281, Loss Enc-Dec: 1.2011\n",
            "Epoch [1/5], Batch [7], Loss D: 0.4399, Loss Enc-Dec: 1.1734\n",
            "Epoch [1/5], Batch [8], Loss D: 0.4733, Loss Enc-Dec: 1.1534\n",
            "Epoch [1/5], Batch [9], Loss D: 0.4886, Loss Enc-Dec: 1.2047\n",
            "Epoch [1/5], Batch [10], Loss D: 0.2951, Loss Enc-Dec: 1.1354\n",
            "Epoch [1/5], Batch [11], Loss D: 0.4920, Loss Enc-Dec: 1.1293\n",
            "Epoch [1/5], Batch [12], Loss D: 0.3840, Loss Enc-Dec: 1.1543\n",
            "Epoch [1/5], Batch [13], Loss D: 0.3874, Loss Enc-Dec: 1.1026\n",
            "Epoch [1/5], Batch [14], Loss D: 0.4539, Loss Enc-Dec: 1.1080\n",
            "Epoch [1/5], Batch [15], Loss D: 0.3736, Loss Enc-Dec: 1.0999\n",
            "Epoch [1/5], Batch [16], Loss D: 0.3378, Loss Enc-Dec: 1.0933\n",
            "Epoch [1/5], Batch [17], Loss D: 0.4570, Loss Enc-Dec: 1.0896\n",
            "Epoch [1/5], Batch [18], Loss D: 0.3803, Loss Enc-Dec: 1.0822\n",
            "Epoch [1/5], Batch [19], Loss D: 0.3573, Loss Enc-Dec: 1.0784\n",
            "Epoch [1/5], Batch [20], Loss D: 0.4283, Loss Enc-Dec: 1.0773\n",
            "Epoch [1/5], Batch [21], Loss D: 0.3927, Loss Enc-Dec: 1.0763\n",
            "Epoch [1/5], Batch [22], Loss D: 0.4270, Loss Enc-Dec: 1.0693\n",
            "Epoch [1/5], Batch [23], Loss D: 0.3647, Loss Enc-Dec: 1.0649\n",
            "Epoch [1/5], Batch [24], Loss D: 0.3701, Loss Enc-Dec: 1.0626\n",
            "Epoch [1/5], Batch [25], Loss D: 0.3681, Loss Enc-Dec: 1.0611\n",
            "Epoch [1/5], Batch [26], Loss D: 0.2829, Loss Enc-Dec: 1.0576\n",
            "Epoch [1/5], Batch [27], Loss D: 0.4297, Loss Enc-Dec: 1.0537\n",
            "Epoch [1/5], Batch [28], Loss D: 0.3414, Loss Enc-Dec: 1.0503\n",
            "Epoch [1/5], Batch [29], Loss D: 0.4473, Loss Enc-Dec: 1.0510\n",
            "Epoch [1/5], Batch [30], Loss D: 0.3212, Loss Enc-Dec: 1.0469\n",
            "Epoch [1/5], Batch [31], Loss D: 0.3655, Loss Enc-Dec: 1.0457\n",
            "Epoch [1/5], Batch [32], Loss D: 0.3495, Loss Enc-Dec: 1.0436\n",
            "Epoch [1/5], Batch [33], Loss D: 0.2120, Loss Enc-Dec: 1.0408\n",
            "Epoch [1/5], Batch [34], Loss D: 0.3486, Loss Enc-Dec: 1.0406\n",
            "Epoch [1/5], Batch [35], Loss D: 0.4758, Loss Enc-Dec: 1.0381\n",
            "Epoch [1/5], Batch [36], Loss D: 0.4246, Loss Enc-Dec: 1.0344\n",
            "Epoch [1/5], Batch [37], Loss D: 0.2900, Loss Enc-Dec: 1.0348\n",
            "Epoch [1/5], Batch [38], Loss D: 0.3825, Loss Enc-Dec: 1.0326\n",
            "Epoch [1/5], Batch [39], Loss D: 0.3061, Loss Enc-Dec: 1.0308\n",
            "Epoch [1/5], Batch [40], Loss D: 0.4920, Loss Enc-Dec: 1.0295\n",
            "Epoch [1/5], Batch [41], Loss D: 0.3276, Loss Enc-Dec: 1.0296\n",
            "Epoch [1/5], Batch [42], Loss D: 0.3744, Loss Enc-Dec: 1.0286\n",
            "Epoch [1/5], Batch [43], Loss D: 0.3543, Loss Enc-Dec: 1.0262\n",
            "Epoch [1/5], Batch [44], Loss D: 0.3526, Loss Enc-Dec: 1.0242\n",
            "Epoch [1/5], Batch [45], Loss D: 0.3567, Loss Enc-Dec: 1.0260\n",
            "Epoch [1/5], Batch [46], Loss D: 0.3395, Loss Enc-Dec: 1.0244\n",
            "Epoch [1/5], Batch [47], Loss D: 0.3962, Loss Enc-Dec: 1.0228\n",
            "Epoch [1/5], Batch [48], Loss D: 0.3121, Loss Enc-Dec: 1.0215\n",
            "Epoch [1/5], Batch [49], Loss D: 0.2820, Loss Enc-Dec: 1.0216\n",
            "Epoch [1/5], Batch [50], Loss D: 0.3390, Loss Enc-Dec: 1.0207\n",
            "Epoch [1/5], Batch [51], Loss D: 0.3896, Loss Enc-Dec: 1.0201\n",
            "Epoch [1/5], Batch [52], Loss D: 0.4788, Loss Enc-Dec: 1.0192\n",
            "Epoch [1/5], Batch [53], Loss D: 0.3317, Loss Enc-Dec: 1.0184\n",
            "Epoch [1/5], Batch [54], Loss D: 0.3424, Loss Enc-Dec: 1.0184\n",
            "Epoch [1/5], Batch [55], Loss D: 0.4120, Loss Enc-Dec: 1.0178\n",
            "Epoch [1/5], Batch [56], Loss D: 0.2246, Loss Enc-Dec: 1.0184\n",
            "Epoch [1/5], Batch [57], Loss D: 0.3316, Loss Enc-Dec: 1.0164\n",
            "Epoch [1/5], Batch [58], Loss D: 0.3865, Loss Enc-Dec: 1.0169\n",
            "Epoch [1/5], Batch [59], Loss D: 0.3348, Loss Enc-Dec: 1.0163\n",
            "Epoch [1/5], Batch [60], Loss D: 0.2908, Loss Enc-Dec: 1.0158\n",
            "Epoch [1/5], Batch [61], Loss D: 0.3596, Loss Enc-Dec: 1.0156\n",
            "Epoch [1/5], Batch [62], Loss D: 0.2716, Loss Enc-Dec: 1.0154\n",
            "Epoch [1/5], Batch [63], Loss D: 0.4201, Loss Enc-Dec: 1.0147\n",
            "Epoch [1/5], Batch [64], Loss D: 0.3682, Loss Enc-Dec: 1.0144\n",
            "Epoch [1/5], Batch [65], Loss D: 0.3418, Loss Enc-Dec: 1.0137\n",
            "Epoch [1/5], Batch [66], Loss D: 0.3331, Loss Enc-Dec: 1.0136\n",
            "Epoch [1/5], Batch [67], Loss D: 0.4266, Loss Enc-Dec: 1.0139\n",
            "Epoch [1/5], Batch [68], Loss D: 0.2790, Loss Enc-Dec: 1.0132\n",
            "Epoch [1/5], Batch [69], Loss D: 0.4028, Loss Enc-Dec: 1.0124\n",
            "Epoch [1/5], Batch [70], Loss D: 0.3585, Loss Enc-Dec: 1.0127\n",
            "Epoch [1/5], Batch [71], Loss D: 0.3759, Loss Enc-Dec: 1.0128\n",
            "Epoch [1/5], Batch [72], Loss D: 0.4048, Loss Enc-Dec: 1.0123\n",
            "Epoch [1/5], Batch [73], Loss D: 0.3274, Loss Enc-Dec: 1.0119\n",
            "Epoch [1/5], Batch [74], Loss D: 0.3882, Loss Enc-Dec: 1.0113\n",
            "Epoch [1/5], Batch [75], Loss D: 0.3285, Loss Enc-Dec: 1.0115\n",
            "Epoch [1/5], Batch [76], Loss D: 0.3120, Loss Enc-Dec: 1.0119\n",
            "Epoch [1/5], Batch [77], Loss D: 0.3311, Loss Enc-Dec: 1.0110\n",
            "Epoch [1/5], Batch [78], Loss D: 0.2882, Loss Enc-Dec: 1.0111\n",
            "Epoch [1/5], Batch [79], Loss D: 0.3143, Loss Enc-Dec: 1.0102\n",
            "Epoch [1/5], Batch [80], Loss D: 0.2718, Loss Enc-Dec: 1.0105\n",
            "Epoch [1/5], Batch [81], Loss D: 0.3117, Loss Enc-Dec: 1.0105\n",
            "Epoch [1/5], Batch [82], Loss D: 0.3061, Loss Enc-Dec: 1.0097\n",
            "Epoch [1/5], Batch [83], Loss D: 0.4071, Loss Enc-Dec: 1.0109\n",
            "Epoch [1/5], Batch [84], Loss D: 0.4166, Loss Enc-Dec: 1.0093\n",
            "Epoch [1/5], Batch [85], Loss D: 0.3402, Loss Enc-Dec: 1.0097\n",
            "Epoch [1/5], Batch [86], Loss D: 0.3811, Loss Enc-Dec: 1.0098\n",
            "Epoch [1/5], Batch [87], Loss D: 0.4031, Loss Enc-Dec: 1.0093\n",
            "Epoch [1/5], Batch [88], Loss D: 0.3541, Loss Enc-Dec: 1.0090\n",
            "Epoch [1/5], Batch [89], Loss D: 0.2475, Loss Enc-Dec: 1.0094\n",
            "Epoch [1/5], Batch [90], Loss D: 0.2922, Loss Enc-Dec: 1.0094\n",
            "Epoch [1/5], Batch [91], Loss D: 0.3453, Loss Enc-Dec: 1.0094\n",
            "Epoch [1/5], Batch [92], Loss D: 0.2741, Loss Enc-Dec: 1.0087\n",
            "Epoch [1/5], Batch [93], Loss D: 0.3137, Loss Enc-Dec: 1.0085\n",
            "Epoch [1/5], Batch [94], Loss D: 0.4029, Loss Enc-Dec: 1.0090\n",
            "Epoch [1/5], Batch [95], Loss D: 0.3566, Loss Enc-Dec: 1.0088\n",
            "Epoch [1/5], Batch [96], Loss D: 0.3824, Loss Enc-Dec: 1.0083\n",
            "Epoch [1/5], Batch [97], Loss D: 0.4145, Loss Enc-Dec: 1.0086\n",
            "Epoch [1/5], Batch [98], Loss D: 0.3785, Loss Enc-Dec: 1.0081\n",
            "Epoch [1/5], Batch [99], Loss D: 0.2936, Loss Enc-Dec: 1.0092\n",
            "Epoch [1/5], Batch [100], Loss D: 0.3760, Loss Enc-Dec: 1.0083\n",
            "Epoch [1/5] completed.\n",
            "Evaluating AAE performance after Epoch 1\n",
            "Mean MCD: 0.5094\n",
            "Epoch [2/5], Batch [1], Loss D: 0.2392, Loss Enc-Dec: 1.0084\n",
            "Epoch [2/5], Batch [2], Loss D: 0.3981, Loss Enc-Dec: 1.0082\n",
            "Epoch [2/5], Batch [3], Loss D: 0.2872, Loss Enc-Dec: 1.0083\n",
            "Epoch [2/5], Batch [4], Loss D: 0.3283, Loss Enc-Dec: 1.0081\n",
            "Epoch [2/5], Batch [5], Loss D: 0.3096, Loss Enc-Dec: 1.0081\n",
            "Epoch [2/5], Batch [6], Loss D: 0.3113, Loss Enc-Dec: 1.0079\n",
            "Epoch [2/5], Batch [7], Loss D: 0.3382, Loss Enc-Dec: 1.0078\n",
            "Epoch [2/5], Batch [8], Loss D: 0.3310, Loss Enc-Dec: 1.0081\n",
            "Epoch [2/5], Batch [9], Loss D: 0.3428, Loss Enc-Dec: 1.0072\n",
            "Epoch [2/5], Batch [10], Loss D: 0.3123, Loss Enc-Dec: 1.0070\n",
            "Epoch [2/5], Batch [11], Loss D: 0.2583, Loss Enc-Dec: 1.0073\n",
            "Epoch [2/5], Batch [12], Loss D: 0.4172, Loss Enc-Dec: 1.0072\n",
            "Epoch [2/5], Batch [13], Loss D: 0.3967, Loss Enc-Dec: 1.0077\n",
            "Epoch [2/5], Batch [14], Loss D: 0.3714, Loss Enc-Dec: 1.0069\n",
            "Epoch [2/5], Batch [15], Loss D: 0.2905, Loss Enc-Dec: 1.0074\n",
            "Epoch [2/5], Batch [16], Loss D: 0.3658, Loss Enc-Dec: 1.0069\n",
            "Epoch [2/5], Batch [17], Loss D: 0.2496, Loss Enc-Dec: 1.0072\n",
            "Epoch [2/5], Batch [18], Loss D: 0.2937, Loss Enc-Dec: 1.0071\n",
            "Epoch [2/5], Batch [19], Loss D: 0.4254, Loss Enc-Dec: 1.0074\n",
            "Epoch [2/5], Batch [20], Loss D: 0.3903, Loss Enc-Dec: 1.0076\n",
            "Epoch [2/5], Batch [21], Loss D: 0.2807, Loss Enc-Dec: 1.0064\n",
            "Epoch [2/5], Batch [22], Loss D: 0.2869, Loss Enc-Dec: 1.0068\n",
            "Epoch [2/5], Batch [23], Loss D: 0.3284, Loss Enc-Dec: 1.0071\n",
            "Epoch [2/5], Batch [24], Loss D: 0.3206, Loss Enc-Dec: 1.0070\n",
            "Epoch [2/5], Batch [25], Loss D: 0.3949, Loss Enc-Dec: 1.0075\n",
            "Epoch [2/5], Batch [26], Loss D: 0.3230, Loss Enc-Dec: 1.0067\n",
            "Epoch [2/5], Batch [27], Loss D: 0.3510, Loss Enc-Dec: 1.0070\n",
            "Epoch [2/5], Batch [28], Loss D: 0.2641, Loss Enc-Dec: 1.0064\n",
            "Epoch [2/5], Batch [29], Loss D: 0.2301, Loss Enc-Dec: 1.0069\n",
            "Epoch [2/5], Batch [30], Loss D: 0.2910, Loss Enc-Dec: 1.0064\n",
            "Epoch [2/5], Batch [31], Loss D: 0.3517, Loss Enc-Dec: 1.0063\n",
            "Epoch [2/5], Batch [32], Loss D: 0.3671, Loss Enc-Dec: 1.0066\n",
            "Epoch [2/5], Batch [33], Loss D: 0.3435, Loss Enc-Dec: 1.0063\n",
            "Epoch [2/5], Batch [34], Loss D: 0.3159, Loss Enc-Dec: 1.0065\n",
            "Epoch [2/5], Batch [35], Loss D: 0.3499, Loss Enc-Dec: 1.0064\n",
            "Epoch [2/5], Batch [36], Loss D: 0.3935, Loss Enc-Dec: 1.0062\n",
            "Epoch [2/5], Batch [37], Loss D: 0.2608, Loss Enc-Dec: 1.0062\n",
            "Epoch [2/5], Batch [38], Loss D: 0.4201, Loss Enc-Dec: 1.0059\n",
            "Epoch [2/5], Batch [39], Loss D: 0.3854, Loss Enc-Dec: 1.0068\n",
            "Epoch [2/5], Batch [40], Loss D: 0.2685, Loss Enc-Dec: 1.0059\n",
            "Epoch [2/5], Batch [41], Loss D: 0.2998, Loss Enc-Dec: 1.0063\n",
            "Epoch [2/5], Batch [42], Loss D: 0.3532, Loss Enc-Dec: 1.0074\n",
            "Epoch [2/5], Batch [43], Loss D: 0.3162, Loss Enc-Dec: 1.0061\n",
            "Epoch [2/5], Batch [44], Loss D: 0.3231, Loss Enc-Dec: 1.0065\n",
            "Epoch [2/5], Batch [45], Loss D: 0.4042, Loss Enc-Dec: 1.0076\n",
            "Epoch [2/5], Batch [46], Loss D: 0.3689, Loss Enc-Dec: 1.0065\n",
            "Epoch [2/5], Batch [47], Loss D: 0.3190, Loss Enc-Dec: 1.0060\n",
            "Epoch [2/5], Batch [48], Loss D: 0.1755, Loss Enc-Dec: 1.0058\n",
            "Epoch [2/5], Batch [49], Loss D: 0.2684, Loss Enc-Dec: 1.0055\n",
            "Epoch [2/5], Batch [50], Loss D: 0.2691, Loss Enc-Dec: 1.0059\n",
            "Epoch [2/5], Batch [51], Loss D: 0.2490, Loss Enc-Dec: 1.0061\n",
            "Epoch [2/5], Batch [52], Loss D: 0.4077, Loss Enc-Dec: 1.0063\n",
            "Epoch [2/5], Batch [53], Loss D: 0.3110, Loss Enc-Dec: 1.0052\n",
            "Epoch [2/5], Batch [54], Loss D: 0.2469, Loss Enc-Dec: 1.0054\n",
            "Epoch [2/5], Batch [55], Loss D: 0.2859, Loss Enc-Dec: 1.0059\n",
            "Epoch [2/5], Batch [56], Loss D: 0.2956, Loss Enc-Dec: 1.0054\n",
            "Epoch [2/5], Batch [57], Loss D: 0.3563, Loss Enc-Dec: 1.0058\n",
            "Epoch [2/5], Batch [58], Loss D: 0.3105, Loss Enc-Dec: 1.0063\n",
            "Epoch [2/5], Batch [59], Loss D: 0.3513, Loss Enc-Dec: 1.0060\n",
            "Epoch [2/5], Batch [60], Loss D: 0.2688, Loss Enc-Dec: 1.0058\n",
            "Epoch [2/5], Batch [61], Loss D: 0.3624, Loss Enc-Dec: 1.0057\n",
            "Epoch [2/5], Batch [62], Loss D: 0.3128, Loss Enc-Dec: 1.0058\n",
            "Epoch [2/5], Batch [63], Loss D: 0.2352, Loss Enc-Dec: 1.0052\n",
            "Epoch [2/5], Batch [64], Loss D: 0.3538, Loss Enc-Dec: 1.0059\n",
            "Epoch [2/5], Batch [65], Loss D: 0.2706, Loss Enc-Dec: 1.0061\n",
            "Epoch [2/5], Batch [66], Loss D: 0.3784, Loss Enc-Dec: 1.0057\n",
            "Epoch [2/5], Batch [67], Loss D: 0.2601, Loss Enc-Dec: 1.0054\n",
            "Epoch [2/5], Batch [68], Loss D: 0.3284, Loss Enc-Dec: 1.0051\n",
            "Epoch [2/5], Batch [69], Loss D: 0.2754, Loss Enc-Dec: 1.0058\n",
            "Epoch [2/5], Batch [70], Loss D: 0.3413, Loss Enc-Dec: 1.0052\n",
            "Epoch [2/5], Batch [71], Loss D: 0.3099, Loss Enc-Dec: 1.0054\n",
            "Epoch [2/5], Batch [72], Loss D: 0.2847, Loss Enc-Dec: 1.0052\n",
            "Epoch [2/5], Batch [73], Loss D: 0.2878, Loss Enc-Dec: 1.0055\n",
            "Epoch [2/5], Batch [74], Loss D: 0.2739, Loss Enc-Dec: 1.0047\n",
            "Epoch [2/5], Batch [75], Loss D: 0.2702, Loss Enc-Dec: 1.0052\n",
            "Epoch [2/5], Batch [76], Loss D: 0.4258, Loss Enc-Dec: 1.0050\n",
            "Epoch [2/5], Batch [77], Loss D: 0.3116, Loss Enc-Dec: 1.0050\n",
            "Epoch [2/5], Batch [78], Loss D: 0.3627, Loss Enc-Dec: 1.0056\n",
            "Epoch [2/5], Batch [79], Loss D: 0.3574, Loss Enc-Dec: 1.0056\n",
            "Epoch [2/5], Batch [80], Loss D: 0.3459, Loss Enc-Dec: 1.0047\n",
            "Epoch [2/5], Batch [81], Loss D: 0.2799, Loss Enc-Dec: 1.0044\n",
            "Epoch [2/5], Batch [82], Loss D: 0.3724, Loss Enc-Dec: 1.0050\n",
            "Epoch [2/5], Batch [83], Loss D: 0.3402, Loss Enc-Dec: 1.0053\n",
            "Epoch [2/5], Batch [84], Loss D: 0.2751, Loss Enc-Dec: 1.0057\n",
            "Epoch [2/5], Batch [85], Loss D: 0.3875, Loss Enc-Dec: 1.0053\n",
            "Epoch [2/5], Batch [86], Loss D: 0.2703, Loss Enc-Dec: 1.0053\n",
            "Epoch [2/5], Batch [87], Loss D: 0.3635, Loss Enc-Dec: 1.0051\n",
            "Epoch [2/5], Batch [88], Loss D: 0.3008, Loss Enc-Dec: 1.0051\n",
            "Epoch [2/5], Batch [89], Loss D: 0.2656, Loss Enc-Dec: 1.0051\n",
            "Epoch [2/5], Batch [90], Loss D: 0.3455, Loss Enc-Dec: 1.0045\n",
            "Epoch [2/5], Batch [91], Loss D: 0.3799, Loss Enc-Dec: 1.0046\n",
            "Epoch [2/5], Batch [92], Loss D: 0.3292, Loss Enc-Dec: 1.0045\n",
            "Epoch [2/5], Batch [93], Loss D: 0.2403, Loss Enc-Dec: 1.0052\n",
            "Epoch [2/5], Batch [94], Loss D: 0.3161, Loss Enc-Dec: 1.0046\n",
            "Epoch [2/5], Batch [95], Loss D: 0.2773, Loss Enc-Dec: 1.0053\n",
            "Epoch [2/5], Batch [96], Loss D: 0.2652, Loss Enc-Dec: 1.0049\n",
            "Epoch [2/5], Batch [97], Loss D: 0.4023, Loss Enc-Dec: 1.0048\n",
            "Epoch [2/5], Batch [98], Loss D: 0.3080, Loss Enc-Dec: 1.0054\n",
            "Epoch [2/5], Batch [99], Loss D: 0.2857, Loss Enc-Dec: 1.0050\n",
            "Epoch [2/5], Batch [100], Loss D: 0.2178, Loss Enc-Dec: 1.0052\n",
            "Epoch [2/5] completed.\n",
            "Evaluating AAE performance after Epoch 2\n",
            "Mean MCD: 0.4947\n",
            "Epoch [3/5], Batch [1], Loss D: 0.2873, Loss Enc-Dec: 1.0047\n",
            "Epoch [3/5], Batch [2], Loss D: 0.2897, Loss Enc-Dec: 1.0043\n",
            "Epoch [3/5], Batch [3], Loss D: 0.2788, Loss Enc-Dec: 1.0044\n",
            "Epoch [3/5], Batch [4], Loss D: 0.2399, Loss Enc-Dec: 1.0042\n",
            "Epoch [3/5], Batch [5], Loss D: 0.2826, Loss Enc-Dec: 1.0050\n",
            "Epoch [3/5], Batch [6], Loss D: 0.2849, Loss Enc-Dec: 1.0044\n",
            "Epoch [3/5], Batch [7], Loss D: 0.2892, Loss Enc-Dec: 1.0049\n",
            "Epoch [3/5], Batch [8], Loss D: 0.3949, Loss Enc-Dec: 1.0048\n",
            "Epoch [3/5], Batch [9], Loss D: 0.3073, Loss Enc-Dec: 1.0041\n",
            "Epoch [3/5], Batch [10], Loss D: 0.2804, Loss Enc-Dec: 1.0042\n",
            "Epoch [3/5], Batch [11], Loss D: 0.2917, Loss Enc-Dec: 1.0046\n",
            "Epoch [3/5], Batch [12], Loss D: 0.2622, Loss Enc-Dec: 1.0046\n",
            "Epoch [3/5], Batch [13], Loss D: 0.3535, Loss Enc-Dec: 1.0038\n",
            "Epoch [3/5], Batch [14], Loss D: 0.2926, Loss Enc-Dec: 1.0043\n",
            "Epoch [3/5], Batch [15], Loss D: 0.2750, Loss Enc-Dec: 1.0043\n",
            "Epoch [3/5], Batch [16], Loss D: 0.3847, Loss Enc-Dec: 1.0040\n",
            "Epoch [3/5], Batch [17], Loss D: 0.2887, Loss Enc-Dec: 1.0045\n",
            "Epoch [3/5], Batch [18], Loss D: 0.2679, Loss Enc-Dec: 1.0047\n",
            "Epoch [3/5], Batch [19], Loss D: 0.3746, Loss Enc-Dec: 1.0046\n",
            "Epoch [3/5], Batch [20], Loss D: 0.2183, Loss Enc-Dec: 1.0041\n",
            "Epoch [3/5], Batch [21], Loss D: 0.3893, Loss Enc-Dec: 1.0042\n",
            "Epoch [3/5], Batch [22], Loss D: 0.3809, Loss Enc-Dec: 1.0036\n",
            "Epoch [3/5], Batch [23], Loss D: 0.2859, Loss Enc-Dec: 1.0044\n",
            "Epoch [3/5], Batch [24], Loss D: 0.3317, Loss Enc-Dec: 1.0043\n",
            "Epoch [3/5], Batch [25], Loss D: 0.3745, Loss Enc-Dec: 1.0040\n",
            "Epoch [3/5], Batch [26], Loss D: 0.3148, Loss Enc-Dec: 1.0036\n",
            "Epoch [3/5], Batch [27], Loss D: 0.2609, Loss Enc-Dec: 1.0043\n",
            "Epoch [3/5], Batch [28], Loss D: 0.3271, Loss Enc-Dec: 1.0037\n",
            "Epoch [3/5], Batch [29], Loss D: 0.3430, Loss Enc-Dec: 1.0038\n",
            "Epoch [3/5], Batch [30], Loss D: 0.3831, Loss Enc-Dec: 1.0045\n",
            "Epoch [3/5], Batch [31], Loss D: 0.2789, Loss Enc-Dec: 1.0040\n",
            "Epoch [3/5], Batch [32], Loss D: 0.2093, Loss Enc-Dec: 1.0042\n",
            "Epoch [3/5], Batch [33], Loss D: 0.3466, Loss Enc-Dec: 1.0038\n",
            "Epoch [3/5], Batch [34], Loss D: 0.3186, Loss Enc-Dec: 1.0041\n",
            "Epoch [3/5], Batch [35], Loss D: 0.2557, Loss Enc-Dec: 1.0043\n",
            "Epoch [3/5], Batch [36], Loss D: 0.3154, Loss Enc-Dec: 1.0038\n",
            "Epoch [3/5], Batch [37], Loss D: 0.3329, Loss Enc-Dec: 1.0043\n",
            "Epoch [3/5], Batch [38], Loss D: 0.3948, Loss Enc-Dec: 1.0046\n",
            "Epoch [3/5], Batch [39], Loss D: 0.3963, Loss Enc-Dec: 1.0039\n",
            "Epoch [3/5], Batch [40], Loss D: 0.2731, Loss Enc-Dec: 1.0038\n",
            "Epoch [3/5], Batch [41], Loss D: 0.2924, Loss Enc-Dec: 1.0039\n",
            "Epoch [3/5], Batch [42], Loss D: 0.3137, Loss Enc-Dec: 1.0044\n",
            "Epoch [3/5], Batch [43], Loss D: 0.3656, Loss Enc-Dec: 1.0045\n",
            "Epoch [3/5], Batch [44], Loss D: 0.2824, Loss Enc-Dec: 1.0040\n",
            "Epoch [3/5], Batch [45], Loss D: 0.2001, Loss Enc-Dec: 1.0044\n",
            "Epoch [3/5], Batch [46], Loss D: 0.3549, Loss Enc-Dec: 1.0046\n",
            "Epoch [3/5], Batch [47], Loss D: 0.3109, Loss Enc-Dec: 1.0044\n",
            "Epoch [3/5], Batch [48], Loss D: 0.3057, Loss Enc-Dec: 1.0049\n",
            "Epoch [3/5], Batch [49], Loss D: 0.2038, Loss Enc-Dec: 1.0042\n",
            "Epoch [3/5], Batch [50], Loss D: 0.2786, Loss Enc-Dec: 1.0040\n",
            "Epoch [3/5], Batch [51], Loss D: 0.3369, Loss Enc-Dec: 1.0041\n",
            "Epoch [3/5], Batch [52], Loss D: 0.2553, Loss Enc-Dec: 1.0041\n",
            "Epoch [3/5], Batch [53], Loss D: 0.3853, Loss Enc-Dec: 1.0037\n",
            "Epoch [3/5], Batch [54], Loss D: 0.2498, Loss Enc-Dec: 1.0034\n",
            "Epoch [3/5], Batch [55], Loss D: 0.2784, Loss Enc-Dec: 1.0040\n",
            "Epoch [3/5], Batch [56], Loss D: 0.3925, Loss Enc-Dec: 1.0043\n",
            "Epoch [3/5], Batch [57], Loss D: 0.1990, Loss Enc-Dec: 1.0042\n",
            "Epoch [3/5], Batch [58], Loss D: 0.2087, Loss Enc-Dec: 1.0038\n",
            "Epoch [3/5], Batch [59], Loss D: 0.3640, Loss Enc-Dec: 1.0040\n",
            "Epoch [3/5], Batch [60], Loss D: 0.2489, Loss Enc-Dec: 1.0043\n",
            "Epoch [3/5], Batch [61], Loss D: 0.3022, Loss Enc-Dec: 1.0042\n",
            "Epoch [3/5], Batch [62], Loss D: 0.3416, Loss Enc-Dec: 1.0044\n",
            "Epoch [3/5], Batch [63], Loss D: 0.3116, Loss Enc-Dec: 1.0039\n",
            "Epoch [3/5], Batch [64], Loss D: 0.2162, Loss Enc-Dec: 1.0044\n",
            "Epoch [3/5], Batch [65], Loss D: 0.1904, Loss Enc-Dec: 1.0042\n",
            "Epoch [3/5], Batch [66], Loss D: 0.3016, Loss Enc-Dec: 1.0038\n",
            "Epoch [3/5], Batch [67], Loss D: 0.2806, Loss Enc-Dec: 1.0039\n",
            "Epoch [3/5], Batch [68], Loss D: 0.4343, Loss Enc-Dec: 1.0039\n",
            "Epoch [3/5], Batch [69], Loss D: 0.2814, Loss Enc-Dec: 1.0037\n",
            "Epoch [3/5], Batch [70], Loss D: 0.3189, Loss Enc-Dec: 1.0031\n",
            "Epoch [3/5], Batch [71], Loss D: 0.3168, Loss Enc-Dec: 1.0037\n",
            "Epoch [3/5], Batch [72], Loss D: 0.2803, Loss Enc-Dec: 1.0041\n",
            "Epoch [3/5], Batch [73], Loss D: 0.2633, Loss Enc-Dec: 1.0038\n",
            "Epoch [3/5], Batch [74], Loss D: 0.2387, Loss Enc-Dec: 1.0044\n",
            "Epoch [3/5], Batch [75], Loss D: 0.4226, Loss Enc-Dec: 1.0037\n",
            "Epoch [3/5], Batch [76], Loss D: 0.3149, Loss Enc-Dec: 1.0044\n",
            "Epoch [3/5], Batch [77], Loss D: 0.3533, Loss Enc-Dec: 1.0043\n",
            "Epoch [3/5], Batch [78], Loss D: 0.3033, Loss Enc-Dec: 1.0030\n",
            "Epoch [3/5], Batch [79], Loss D: 0.2576, Loss Enc-Dec: 1.0042\n",
            "Epoch [3/5], Batch [80], Loss D: 0.3933, Loss Enc-Dec: 1.0035\n",
            "Epoch [3/5], Batch [81], Loss D: 0.2633, Loss Enc-Dec: 1.0044\n",
            "Epoch [3/5], Batch [82], Loss D: 0.2740, Loss Enc-Dec: 1.0043\n",
            "Epoch [3/5], Batch [83], Loss D: 0.2973, Loss Enc-Dec: 1.0034\n",
            "Epoch [3/5], Batch [84], Loss D: 0.3495, Loss Enc-Dec: 1.0042\n",
            "Epoch [3/5], Batch [85], Loss D: 0.4031, Loss Enc-Dec: 1.0042\n",
            "Epoch [3/5], Batch [86], Loss D: 0.3154, Loss Enc-Dec: 1.0033\n",
            "Epoch [3/5], Batch [87], Loss D: 0.2724, Loss Enc-Dec: 1.0038\n",
            "Epoch [3/5], Batch [88], Loss D: 0.2294, Loss Enc-Dec: 1.0039\n",
            "Epoch [3/5], Batch [89], Loss D: 0.3213, Loss Enc-Dec: 1.0039\n",
            "Epoch [3/5], Batch [90], Loss D: 0.3622, Loss Enc-Dec: 1.0035\n",
            "Epoch [3/5], Batch [91], Loss D: 0.2418, Loss Enc-Dec: 1.0041\n",
            "Epoch [3/5], Batch [92], Loss D: 0.1826, Loss Enc-Dec: 1.0042\n",
            "Epoch [3/5], Batch [93], Loss D: 0.1916, Loss Enc-Dec: 1.0038\n",
            "Epoch [3/5], Batch [94], Loss D: 0.2444, Loss Enc-Dec: 1.0040\n",
            "Epoch [3/5], Batch [95], Loss D: 0.2974, Loss Enc-Dec: 1.0042\n",
            "Epoch [3/5], Batch [96], Loss D: 0.2196, Loss Enc-Dec: 1.0047\n",
            "Epoch [3/5], Batch [97], Loss D: 0.3860, Loss Enc-Dec: 1.0036\n",
            "Epoch [3/5], Batch [98], Loss D: 0.3163, Loss Enc-Dec: 1.0038\n",
            "Epoch [3/5], Batch [99], Loss D: 0.2859, Loss Enc-Dec: 1.0039\n",
            "Epoch [3/5], Batch [100], Loss D: 0.3013, Loss Enc-Dec: 1.0040\n",
            "Epoch [3/5] completed.\n",
            "Evaluating AAE performance after Epoch 3\n",
            "Mean MCD: 0.4720\n",
            "Epoch [4/5], Batch [1], Loss D: 0.2614, Loss Enc-Dec: 1.0030\n",
            "Epoch [4/5], Batch [2], Loss D: 0.2602, Loss Enc-Dec: 1.0037\n",
            "Epoch [4/5], Batch [3], Loss D: 0.2323, Loss Enc-Dec: 1.0041\n",
            "Epoch [4/5], Batch [4], Loss D: 0.2648, Loss Enc-Dec: 1.0040\n",
            "Epoch [4/5], Batch [5], Loss D: 0.2631, Loss Enc-Dec: 1.0042\n",
            "Epoch [4/5], Batch [6], Loss D: 0.2422, Loss Enc-Dec: 1.0044\n",
            "Epoch [4/5], Batch [7], Loss D: 0.3536, Loss Enc-Dec: 1.0043\n",
            "Epoch [4/5], Batch [8], Loss D: 0.2187, Loss Enc-Dec: 1.0042\n",
            "Epoch [4/5], Batch [9], Loss D: 0.1447, Loss Enc-Dec: 1.0042\n",
            "Epoch [4/5], Batch [10], Loss D: 0.2648, Loss Enc-Dec: 1.0038\n",
            "Epoch [4/5], Batch [11], Loss D: 0.2474, Loss Enc-Dec: 1.0041\n",
            "Epoch [4/5], Batch [12], Loss D: 0.2596, Loss Enc-Dec: 1.0042\n",
            "Epoch [4/5], Batch [13], Loss D: 0.3363, Loss Enc-Dec: 1.0036\n",
            "Epoch [4/5], Batch [14], Loss D: 0.2997, Loss Enc-Dec: 1.0040\n",
            "Epoch [4/5], Batch [15], Loss D: 0.2089, Loss Enc-Dec: 1.0040\n",
            "Epoch [4/5], Batch [16], Loss D: 0.3334, Loss Enc-Dec: 1.0030\n",
            "Epoch [4/5], Batch [17], Loss D: 0.2310, Loss Enc-Dec: 1.0037\n",
            "Epoch [4/5], Batch [18], Loss D: 0.3322, Loss Enc-Dec: 1.0040\n",
            "Epoch [4/5], Batch [19], Loss D: 0.4960, Loss Enc-Dec: 1.0041\n",
            "Epoch [4/5], Batch [20], Loss D: 0.2651, Loss Enc-Dec: 1.0036\n",
            "Epoch [4/5], Batch [21], Loss D: 0.3156, Loss Enc-Dec: 1.0041\n",
            "Epoch [4/5], Batch [22], Loss D: 0.2868, Loss Enc-Dec: 1.0038\n",
            "Epoch [4/5], Batch [23], Loss D: 0.2049, Loss Enc-Dec: 1.0042\n",
            "Epoch [4/5], Batch [24], Loss D: 0.3253, Loss Enc-Dec: 1.0036\n",
            "Epoch [4/5], Batch [25], Loss D: 0.2634, Loss Enc-Dec: 1.0037\n",
            "Epoch [4/5], Batch [26], Loss D: 0.3296, Loss Enc-Dec: 1.0048\n",
            "Epoch [4/5], Batch [27], Loss D: 0.2334, Loss Enc-Dec: 1.0036\n",
            "Epoch [4/5], Batch [28], Loss D: 0.3322, Loss Enc-Dec: 1.0040\n",
            "Epoch [4/5], Batch [29], Loss D: 0.2646, Loss Enc-Dec: 1.0039\n",
            "Epoch [4/5], Batch [30], Loss D: 0.2783, Loss Enc-Dec: 1.0042\n",
            "Epoch [4/5], Batch [31], Loss D: 0.2245, Loss Enc-Dec: 1.0037\n",
            "Epoch [4/5], Batch [32], Loss D: 0.2876, Loss Enc-Dec: 1.0034\n",
            "Epoch [4/5], Batch [33], Loss D: 0.2520, Loss Enc-Dec: 1.0038\n",
            "Epoch [4/5], Batch [34], Loss D: 0.2406, Loss Enc-Dec: 1.0040\n",
            "Epoch [4/5], Batch [35], Loss D: 0.2962, Loss Enc-Dec: 1.0039\n",
            "Epoch [4/5], Batch [36], Loss D: 0.2642, Loss Enc-Dec: 1.0037\n",
            "Epoch [4/5], Batch [37], Loss D: 0.2656, Loss Enc-Dec: 1.0037\n",
            "Epoch [4/5], Batch [38], Loss D: 0.2271, Loss Enc-Dec: 1.0033\n",
            "Epoch [4/5], Batch [39], Loss D: 0.2694, Loss Enc-Dec: 1.0037\n",
            "Epoch [4/5], Batch [40], Loss D: 0.2673, Loss Enc-Dec: 1.0035\n",
            "Epoch [4/5], Batch [41], Loss D: 0.1765, Loss Enc-Dec: 1.0028\n",
            "Epoch [4/5], Batch [42], Loss D: 0.3295, Loss Enc-Dec: 1.0034\n",
            "Epoch [4/5], Batch [43], Loss D: 0.3221, Loss Enc-Dec: 1.0039\n",
            "Epoch [4/5], Batch [44], Loss D: 0.1963, Loss Enc-Dec: 1.0031\n",
            "Epoch [4/5], Batch [45], Loss D: 0.3083, Loss Enc-Dec: 1.0033\n",
            "Epoch [4/5], Batch [46], Loss D: 0.2540, Loss Enc-Dec: 1.0026\n",
            "Epoch [4/5], Batch [47], Loss D: 0.2966, Loss Enc-Dec: 1.0036\n",
            "Epoch [4/5], Batch [48], Loss D: 0.2693, Loss Enc-Dec: 1.0033\n",
            "Epoch [4/5], Batch [49], Loss D: 0.3277, Loss Enc-Dec: 1.0034\n",
            "Epoch [4/5], Batch [50], Loss D: 0.1907, Loss Enc-Dec: 1.0037\n",
            "Epoch [4/5], Batch [51], Loss D: 0.2173, Loss Enc-Dec: 1.0045\n",
            "Epoch [4/5], Batch [52], Loss D: 0.2933, Loss Enc-Dec: 1.0040\n",
            "Epoch [4/5], Batch [53], Loss D: 0.2871, Loss Enc-Dec: 1.0033\n",
            "Epoch [4/5], Batch [54], Loss D: 0.3076, Loss Enc-Dec: 1.0033\n",
            "Epoch [4/5], Batch [55], Loss D: 0.2407, Loss Enc-Dec: 1.0035\n",
            "Epoch [4/5], Batch [56], Loss D: 0.3613, Loss Enc-Dec: 1.0040\n",
            "Epoch [4/5], Batch [57], Loss D: 0.3033, Loss Enc-Dec: 1.0037\n",
            "Epoch [4/5], Batch [58], Loss D: 0.2730, Loss Enc-Dec: 1.0033\n",
            "Epoch [4/5], Batch [59], Loss D: 0.1939, Loss Enc-Dec: 1.0036\n",
            "Epoch [4/5], Batch [60], Loss D: 0.2480, Loss Enc-Dec: 1.0034\n",
            "Epoch [4/5], Batch [61], Loss D: 0.3176, Loss Enc-Dec: 1.0039\n",
            "Epoch [4/5], Batch [62], Loss D: 0.2698, Loss Enc-Dec: 1.0034\n",
            "Epoch [4/5], Batch [63], Loss D: 0.3141, Loss Enc-Dec: 1.0029\n",
            "Epoch [4/5], Batch [64], Loss D: 0.3101, Loss Enc-Dec: 1.0040\n",
            "Epoch [4/5], Batch [65], Loss D: 0.3844, Loss Enc-Dec: 1.0036\n",
            "Epoch [4/5], Batch [66], Loss D: 0.2750, Loss Enc-Dec: 1.0031\n",
            "Epoch [4/5], Batch [67], Loss D: 0.2995, Loss Enc-Dec: 1.0038\n",
            "Epoch [4/5], Batch [68], Loss D: 0.1695, Loss Enc-Dec: 1.0029\n",
            "Epoch [4/5], Batch [69], Loss D: 0.3723, Loss Enc-Dec: 1.0036\n",
            "Epoch [4/5], Batch [70], Loss D: 0.2794, Loss Enc-Dec: 1.0039\n",
            "Epoch [4/5], Batch [71], Loss D: 0.2623, Loss Enc-Dec: 1.0045\n",
            "Epoch [4/5], Batch [72], Loss D: 0.1690, Loss Enc-Dec: 1.0039\n",
            "Epoch [4/5], Batch [73], Loss D: 0.3467, Loss Enc-Dec: 1.0037\n",
            "Epoch [4/5], Batch [74], Loss D: 0.2621, Loss Enc-Dec: 1.0030\n",
            "Epoch [4/5], Batch [75], Loss D: 0.3172, Loss Enc-Dec: 1.0037\n",
            "Epoch [4/5], Batch [76], Loss D: 0.2191, Loss Enc-Dec: 1.0037\n",
            "Epoch [4/5], Batch [77], Loss D: 0.3100, Loss Enc-Dec: 1.0039\n",
            "Epoch [4/5], Batch [78], Loss D: 0.2697, Loss Enc-Dec: 1.0038\n",
            "Epoch [4/5], Batch [79], Loss D: 0.3524, Loss Enc-Dec: 1.0037\n",
            "Epoch [4/5], Batch [80], Loss D: 0.2610, Loss Enc-Dec: 1.0035\n",
            "Epoch [4/5], Batch [81], Loss D: 0.4210, Loss Enc-Dec: 1.0036\n",
            "Epoch [4/5], Batch [82], Loss D: 0.1434, Loss Enc-Dec: 1.0034\n",
            "Epoch [4/5], Batch [83], Loss D: 0.3053, Loss Enc-Dec: 1.0039\n",
            "Epoch [4/5], Batch [84], Loss D: 0.1909, Loss Enc-Dec: 1.0034\n",
            "Epoch [4/5], Batch [85], Loss D: 0.2243, Loss Enc-Dec: 1.0038\n",
            "Epoch [4/5], Batch [86], Loss D: 0.2046, Loss Enc-Dec: 1.0040\n",
            "Epoch [4/5], Batch [87], Loss D: 0.3608, Loss Enc-Dec: 1.0042\n",
            "Epoch [4/5], Batch [88], Loss D: 0.2074, Loss Enc-Dec: 1.0034\n",
            "Epoch [4/5], Batch [89], Loss D: 0.2969, Loss Enc-Dec: 1.0032\n",
            "Epoch [4/5], Batch [90], Loss D: 0.2382, Loss Enc-Dec: 1.0039\n",
            "Epoch [4/5], Batch [91], Loss D: 0.1569, Loss Enc-Dec: 1.0036\n",
            "Epoch [4/5], Batch [92], Loss D: 0.2357, Loss Enc-Dec: 1.0039\n",
            "Epoch [4/5], Batch [93], Loss D: 0.2942, Loss Enc-Dec: 1.0035\n",
            "Epoch [4/5], Batch [94], Loss D: 0.2468, Loss Enc-Dec: 1.0034\n",
            "Epoch [4/5], Batch [95], Loss D: 0.1739, Loss Enc-Dec: 1.0034\n",
            "Epoch [4/5], Batch [96], Loss D: 0.2414, Loss Enc-Dec: 1.0037\n",
            "Epoch [4/5], Batch [97], Loss D: 0.2102, Loss Enc-Dec: 1.0031\n",
            "Epoch [4/5], Batch [98], Loss D: 0.2172, Loss Enc-Dec: 1.0034\n",
            "Epoch [4/5], Batch [99], Loss D: 0.2167, Loss Enc-Dec: 1.0036\n",
            "Epoch [4/5], Batch [100], Loss D: 0.1207, Loss Enc-Dec: 1.0043\n",
            "Epoch [4/5] completed.\n",
            "Evaluating AAE performance after Epoch 4\n",
            "Mean MCD: 0.5238\n",
            "Epoch [5/5], Batch [1], Loss D: 0.2730, Loss Enc-Dec: 1.0035\n",
            "Epoch [5/5], Batch [2], Loss D: 0.3488, Loss Enc-Dec: 1.0037\n",
            "Epoch [5/5], Batch [3], Loss D: 0.2635, Loss Enc-Dec: 1.0035\n",
            "Epoch [5/5], Batch [4], Loss D: 0.3377, Loss Enc-Dec: 1.0043\n",
            "Epoch [5/5], Batch [5], Loss D: 0.3029, Loss Enc-Dec: 1.0033\n",
            "Epoch [5/5], Batch [6], Loss D: 0.1804, Loss Enc-Dec: 1.0027\n",
            "Epoch [5/5], Batch [7], Loss D: 0.3191, Loss Enc-Dec: 1.0040\n",
            "Epoch [5/5], Batch [8], Loss D: 0.2901, Loss Enc-Dec: 1.0033\n",
            "Epoch [5/5], Batch [9], Loss D: 0.2137, Loss Enc-Dec: 1.0029\n",
            "Epoch [5/5], Batch [10], Loss D: 0.2468, Loss Enc-Dec: 1.0037\n",
            "Epoch [5/5], Batch [11], Loss D: 0.1849, Loss Enc-Dec: 1.0032\n",
            "Epoch [5/5], Batch [12], Loss D: 0.2833, Loss Enc-Dec: 1.0029\n",
            "Epoch [5/5], Batch [13], Loss D: 0.2482, Loss Enc-Dec: 1.0035\n",
            "Epoch [5/5], Batch [14], Loss D: 0.1786, Loss Enc-Dec: 1.0032\n",
            "Epoch [5/5], Batch [15], Loss D: 0.2527, Loss Enc-Dec: 1.0032\n",
            "Epoch [5/5], Batch [16], Loss D: 0.2625, Loss Enc-Dec: 1.0034\n",
            "Epoch [5/5], Batch [17], Loss D: 0.2690, Loss Enc-Dec: 1.0033\n",
            "Epoch [5/5], Batch [18], Loss D: 0.3063, Loss Enc-Dec: 1.0037\n",
            "Epoch [5/5], Batch [19], Loss D: 0.2385, Loss Enc-Dec: 1.0035\n",
            "Epoch [5/5], Batch [20], Loss D: 0.2838, Loss Enc-Dec: 1.0034\n",
            "Epoch [5/5], Batch [21], Loss D: 0.1760, Loss Enc-Dec: 1.0037\n",
            "Epoch [5/5], Batch [22], Loss D: 0.2899, Loss Enc-Dec: 1.0034\n",
            "Epoch [5/5], Batch [23], Loss D: 0.1397, Loss Enc-Dec: 1.0031\n",
            "Epoch [5/5], Batch [24], Loss D: 0.2896, Loss Enc-Dec: 1.0029\n",
            "Epoch [5/5], Batch [25], Loss D: 0.2644, Loss Enc-Dec: 1.0033\n",
            "Epoch [5/5], Batch [26], Loss D: 0.2535, Loss Enc-Dec: 1.0035\n",
            "Epoch [5/5], Batch [27], Loss D: 0.2935, Loss Enc-Dec: 1.0023\n",
            "Epoch [5/5], Batch [28], Loss D: 0.2352, Loss Enc-Dec: 1.0035\n",
            "Epoch [5/5], Batch [29], Loss D: 0.2878, Loss Enc-Dec: 1.0026\n",
            "Epoch [5/5], Batch [30], Loss D: 0.2072, Loss Enc-Dec: 1.0027\n",
            "Epoch [5/5], Batch [31], Loss D: 0.1736, Loss Enc-Dec: 1.0038\n",
            "Epoch [5/5], Batch [32], Loss D: 0.2513, Loss Enc-Dec: 1.0026\n",
            "Epoch [5/5], Batch [33], Loss D: 0.3243, Loss Enc-Dec: 1.0039\n",
            "Epoch [5/5], Batch [34], Loss D: 0.2058, Loss Enc-Dec: 1.0026\n",
            "Epoch [5/5], Batch [35], Loss D: 0.2155, Loss Enc-Dec: 1.0031\n",
            "Epoch [5/5], Batch [36], Loss D: 0.2448, Loss Enc-Dec: 1.0033\n",
            "Epoch [5/5], Batch [37], Loss D: 0.3612, Loss Enc-Dec: 1.0032\n",
            "Epoch [5/5], Batch [38], Loss D: 0.2494, Loss Enc-Dec: 1.0033\n",
            "Epoch [5/5], Batch [39], Loss D: 0.2576, Loss Enc-Dec: 1.0032\n",
            "Epoch [5/5], Batch [40], Loss D: 0.3055, Loss Enc-Dec: 1.0033\n",
            "Epoch [5/5], Batch [41], Loss D: 0.4061, Loss Enc-Dec: 1.0030\n",
            "Epoch [5/5], Batch [42], Loss D: 0.2978, Loss Enc-Dec: 1.0035\n",
            "Epoch [5/5], Batch [43], Loss D: 0.2359, Loss Enc-Dec: 1.0035\n",
            "Epoch [5/5], Batch [44], Loss D: 0.3131, Loss Enc-Dec: 1.0029\n",
            "Epoch [5/5], Batch [45], Loss D: 0.1996, Loss Enc-Dec: 1.0036\n",
            "Epoch [5/5], Batch [46], Loss D: 0.2337, Loss Enc-Dec: 1.0030\n",
            "Epoch [5/5], Batch [47], Loss D: 0.1895, Loss Enc-Dec: 1.0030\n",
            "Epoch [5/5], Batch [48], Loss D: 0.2861, Loss Enc-Dec: 1.0030\n",
            "Epoch [5/5], Batch [49], Loss D: 0.2325, Loss Enc-Dec: 1.0029\n",
            "Epoch [5/5], Batch [50], Loss D: 0.1697, Loss Enc-Dec: 1.0030\n",
            "Epoch [5/5], Batch [51], Loss D: 0.2763, Loss Enc-Dec: 1.0035\n",
            "Epoch [5/5], Batch [52], Loss D: 0.3023, Loss Enc-Dec: 1.0029\n",
            "Epoch [5/5], Batch [53], Loss D: 0.2569, Loss Enc-Dec: 1.0030\n",
            "Epoch [5/5], Batch [54], Loss D: 0.2084, Loss Enc-Dec: 1.0030\n",
            "Epoch [5/5], Batch [55], Loss D: 0.2430, Loss Enc-Dec: 1.0039\n",
            "Epoch [5/5], Batch [56], Loss D: 0.2444, Loss Enc-Dec: 1.0035\n",
            "Epoch [5/5], Batch [57], Loss D: 0.1911, Loss Enc-Dec: 1.0033\n",
            "Epoch [5/5], Batch [58], Loss D: 0.2690, Loss Enc-Dec: 1.0032\n",
            "Epoch [5/5], Batch [59], Loss D: 0.3309, Loss Enc-Dec: 1.0029\n",
            "Epoch [5/5], Batch [60], Loss D: 0.3285, Loss Enc-Dec: 1.0034\n",
            "Epoch [5/5], Batch [61], Loss D: 0.1777, Loss Enc-Dec: 1.0032\n",
            "Epoch [5/5], Batch [62], Loss D: 0.1444, Loss Enc-Dec: 1.0035\n",
            "Epoch [5/5], Batch [63], Loss D: 0.2939, Loss Enc-Dec: 1.0035\n",
            "Epoch [5/5], Batch [64], Loss D: 0.2565, Loss Enc-Dec: 1.0031\n",
            "Epoch [5/5], Batch [65], Loss D: 0.2613, Loss Enc-Dec: 1.0036\n",
            "Epoch [5/5], Batch [66], Loss D: 0.2404, Loss Enc-Dec: 1.0033\n",
            "Epoch [5/5], Batch [67], Loss D: 0.1990, Loss Enc-Dec: 1.0030\n",
            "Epoch [5/5], Batch [68], Loss D: 0.2002, Loss Enc-Dec: 1.0032\n",
            "Epoch [5/5], Batch [69], Loss D: 0.2653, Loss Enc-Dec: 1.0029\n",
            "Epoch [5/5], Batch [70], Loss D: 0.3157, Loss Enc-Dec: 1.0033\n",
            "Epoch [5/5], Batch [71], Loss D: 0.2759, Loss Enc-Dec: 1.0030\n",
            "Epoch [5/5], Batch [72], Loss D: 0.3102, Loss Enc-Dec: 1.0038\n",
            "Epoch [5/5], Batch [73], Loss D: 0.2723, Loss Enc-Dec: 1.0039\n",
            "Epoch [5/5], Batch [74], Loss D: 0.1445, Loss Enc-Dec: 1.0031\n",
            "Epoch [5/5], Batch [75], Loss D: 0.3212, Loss Enc-Dec: 1.0034\n",
            "Epoch [5/5], Batch [76], Loss D: 0.2277, Loss Enc-Dec: 1.0030\n",
            "Epoch [5/5], Batch [77], Loss D: 0.2167, Loss Enc-Dec: 1.0031\n",
            "Epoch [5/5], Batch [78], Loss D: 0.1730, Loss Enc-Dec: 1.0034\n",
            "Epoch [5/5], Batch [79], Loss D: 0.1574, Loss Enc-Dec: 1.0030\n",
            "Epoch [5/5], Batch [80], Loss D: 0.2513, Loss Enc-Dec: 1.0030\n",
            "Epoch [5/5], Batch [81], Loss D: 0.2129, Loss Enc-Dec: 1.0034\n",
            "Epoch [5/5], Batch [82], Loss D: 0.3129, Loss Enc-Dec: 1.0028\n",
            "Epoch [5/5], Batch [83], Loss D: 0.2576, Loss Enc-Dec: 1.0032\n",
            "Epoch [5/5], Batch [84], Loss D: 0.2306, Loss Enc-Dec: 1.0030\n",
            "Epoch [5/5], Batch [85], Loss D: 0.2504, Loss Enc-Dec: 1.0028\n",
            "Epoch [5/5], Batch [86], Loss D: 0.2885, Loss Enc-Dec: 1.0032\n",
            "Epoch [5/5], Batch [87], Loss D: 0.2144, Loss Enc-Dec: 1.0032\n",
            "Epoch [5/5], Batch [88], Loss D: 0.1978, Loss Enc-Dec: 1.0033\n",
            "Epoch [5/5], Batch [89], Loss D: 0.3395, Loss Enc-Dec: 1.0029\n",
            "Epoch [5/5], Batch [90], Loss D: 0.2628, Loss Enc-Dec: 1.0026\n",
            "Epoch [5/5], Batch [91], Loss D: 0.1791, Loss Enc-Dec: 1.0035\n",
            "Epoch [5/5], Batch [92], Loss D: 0.2608, Loss Enc-Dec: 1.0033\n",
            "Epoch [5/5], Batch [93], Loss D: 0.3025, Loss Enc-Dec: 1.0028\n",
            "Epoch [5/5], Batch [94], Loss D: 0.2207, Loss Enc-Dec: 1.0022\n",
            "Epoch [5/5], Batch [95], Loss D: 0.2394, Loss Enc-Dec: 1.0031\n",
            "Epoch [5/5], Batch [96], Loss D: 0.2262, Loss Enc-Dec: 1.0027\n",
            "Epoch [5/5], Batch [97], Loss D: 0.1476, Loss Enc-Dec: 1.0030\n",
            "Epoch [5/5], Batch [98], Loss D: 0.2716, Loss Enc-Dec: 1.0031\n",
            "Epoch [5/5], Batch [99], Loss D: 0.2238, Loss Enc-Dec: 1.0030\n",
            "Epoch [5/5], Batch [100], Loss D: 0.2967, Loss Enc-Dec: 1.0035\n",
            "Epoch [5/5] completed.\n",
            "Evaluating AAE performance after Epoch 5\n",
            "Mean MCD: 0.4926\n",
            "Training completed successfully!\n"
          ]
        }
      ]
    }
  ]
}