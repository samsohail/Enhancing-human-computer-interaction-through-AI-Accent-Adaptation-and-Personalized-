{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VHkWdPQKtSE",
        "outputId": "bd9c4280-5c27-49f3-e6ee-5b8fbdd43511"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchaudio\n",
        "!pip install audiomentations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GJdc4WRLKMS",
        "outputId": "efdd36ca-d321-43a5-db37-cae4185c8732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: torch==2.4.0 in /usr/local/lib/python3.10/dist-packages (from torchaudio) (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchaudio) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchaudio) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchaudio) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchaudio) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchaudio) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchaudio) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0->torchaudio) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0->torchaudio) (1.3.0)\n",
            "Collecting audiomentations\n",
            "  Downloading audiomentations-0.37.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from audiomentations) (1.26.4)\n",
            "Collecting numpy-minmax<1,>=0.3.0 (from audiomentations)\n",
            "  Downloading numpy_minmax-0.3.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting numpy-rms<1,>=0.4.2 (from audiomentations)\n",
            "  Downloading numpy_rms-0.4.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: librosa!=0.10.0,<0.11.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from audiomentations) (0.10.2.post1)\n",
            "Collecting scipy<1.13,>=1.4 (from audiomentations)\n",
            "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soxr<1.0.0,>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from audiomentations) (0.5.0.post1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.3.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.8.2)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.0.8)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from numpy-minmax<1,>=0.3.0->audiomentations) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.0->numpy-minmax<1,>=0.3.0->audiomentations) (2.22)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (24.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (4.3.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2024.8.30)\n",
            "Downloading audiomentations-0.37.0-py3-none-any.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.5/80.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy_minmax-0.3.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading numpy_rms-0.4.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17 kB)\n",
            "Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy, numpy-rms, numpy-minmax, audiomentations\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "osqp 0.6.7.post0 requires scipy!=1.12.0,>=0.13.2, but you have scipy 1.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed audiomentations-0.37.0 numpy-minmax-0.3.1 numpy-rms-0.4.2 scipy-1.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "from torch.nn import functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import write\n",
        "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n",
        "\n",
        "# Step 1: Define Data Augmentation Pipeline\n",
        "augment = Compose([\n",
        "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
        "    TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n",
        "    PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
        "    Shift(min_shift=-0.5, max_shift=0.5, p=0.5)\n",
        "])\n",
        "\n",
        "# Custom Dataset for Loading Audio Files with Augmentation\n",
        "class SpeechDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None, target_length=80000, num_files=100, apply_augmentation=False):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.target_length = target_length\n",
        "        self.apply_augmentation = apply_augmentation\n",
        "        self.audio_files = [f for f in os.listdir(data_dir) if f.endswith('.wav')][:num_files]\n",
        "\n",
        "        if len(self.audio_files) == 0:\n",
        "            raise ValueError(f\"No audio files found in directory: {data_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        wav_path = os.path.join(self.data_dir, self.audio_files[idx])\n",
        "        if not os.path.exists(wav_path):\n",
        "            raise FileNotFoundError(f\"Audio file not found: {wav_path}\")\n",
        "\n",
        "        try:\n",
        "            waveform, sr = torchaudio.load(wav_path)\n",
        "        except RuntimeError as e:\n",
        "            print(f\"Error loading audio file: {wav_path}, Error: {e}\")\n",
        "            raise e\n",
        "\n",
        "        # Pad or truncate to the target length\n",
        "        if waveform.shape[1] < self.target_length:\n",
        "            padding = self.target_length - waveform.shape[1]\n",
        "            waveform = F.pad(waveform, (0, padding))\n",
        "        else:\n",
        "            waveform = waveform[:, :self.target_length]\n",
        "\n",
        "        # Apply Augmentation\n",
        "        if self.apply_augmentation:\n",
        "            waveform = augment_audio(waveform)\n",
        "\n",
        "        # Apply normalization if any\n",
        "        if self.transform:\n",
        "            waveform = self.transform(waveform)\n",
        "\n",
        "        return waveform, self.audio_files[idx]\n",
        "\n",
        "# Function to Apply Augmentation to the Audio\n",
        "def augment_audio(audio):\n",
        "    augmented_samples = augment(samples=audio.numpy(), sample_rate=16000)\n",
        "    return torch.tensor(augmented_samples)\n",
        "\n",
        "# Define Normalization Transform\n",
        "def normalize_waveform(waveform):\n",
        "    return (waveform - waveform.mean()) / waveform.std()\n",
        "\n",
        "# Directories for Data\n",
        "data_dir_A = \"/content/drive/MyDrive/data/extracted_files-3/en/North_American_English_W/\"\n",
        "\n",
        "# Target length for all audio files\n",
        "target_length = 80000\n",
        "\n",
        "# Initialize Dataset with 100 Files and Apply Augmentation\n",
        "dataset_A = SpeechDataset(data_dir_A, transform=normalize_waveform, target_length=target_length, num_files=100, apply_augmentation=True)\n",
        "\n",
        "# Initialize DataLoader\n",
        "dataloader_A = DataLoader(dataset_A, batch_size=1, shuffle=True)\n",
        "\n",
        "# Define VAE without Transformer Components: Encoder, Decoder\n",
        "class SimpleVAE(nn.Module):\n",
        "    def __init__(self, input_dim=80000, latent_dim=64):\n",
        "        super(SimpleVAE, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 512)\n",
        "        self.fc21 = nn.Linear(512, latent_dim)\n",
        "        self.fc22 = nn.Linear(512, latent_dim)\n",
        "        self.fc3 = nn.Linear(latent_dim, 512)\n",
        "        self.fc4 = nn.Linear(512, input_dim)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.fc1(x.view(-1, 80000)))\n",
        "        return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "    def loss_function(self, recon_x, x, mu, logvar):\n",
        "        recon_loss = nn.MSELoss()(recon_x, x.view(-1, 80000))\n",
        "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "        return recon_loss + kl_loss\n",
        "\n",
        "# Initialize Models\n",
        "model = SimpleVAE()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0002)\n",
        "\n",
        "# Set device to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Function to save audio data correctly\n",
        "def save_audio(file_path, audio_tensor, sample_rate=16000):\n",
        "    audio_np = audio_tensor.cpu().detach().numpy().squeeze(0)\n",
        "    audio_np = audio_np / np.max(np.abs(audio_np) + 1e-6)\n",
        "    audio_np = np.clip(audio_np, -1, 1)\n",
        "    audio_np = (audio_np * 32767).astype(np.int16)\n",
        "\n",
        "    if len(audio_np.shape) > 1:\n",
        "        audio_np = audio_np[0]\n",
        "\n",
        "    write(file_path, sample_rate, audio_np)\n",
        "\n",
        "# Training Loop for Simple VAE\n",
        "def train_vae(dataloader_A, num_epochs=5):\n",
        "    filename_mapping_A = {}  # Mapping for evaluation\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        batch_count = 0\n",
        "        for real_A, file_A in dataloader_A:\n",
        "            real_A = real_A.to(device)\n",
        "            batch_count += 1\n",
        "\n",
        "            # Forward pass through VAE\n",
        "            reconstructed_A, mu, logvar = model(real_A)\n",
        "\n",
        "            # Compute Loss\n",
        "            loss = model.loss_function(reconstructed_A, real_A, mu, logvar)\n",
        "\n",
        "            # Update Model\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Save Converted Audio for Evaluation\n",
        "            converted_A_path = f\"/content/drive/MyDrive/data/extracted_files-3/en/vae_converted_epoch_{epoch}/{file_A[0]}\"\n",
        "            os.makedirs(os.path.dirname(converted_A_path), exist_ok=True)\n",
        "\n",
        "            save_audio(converted_A_path, reconstructed_A, sample_rate=16000)\n",
        "\n",
        "            # Update Mappings for Evaluation\n",
        "            filename_mapping_A[file_A[0]] = file_A[0]  # Map original to new\n",
        "\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_count}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] completed.\")\n",
        "        evaluate_vae(epoch, filename_mapping_A)\n",
        "\n",
        "    print(\"Training completed successfully!\")\n",
        "\n",
        "# Evaluation function for VAE\n",
        "def evaluate_vae(epoch, mapping_A):\n",
        "    original_dir_A = \"/content/drive/MyDrive/data/extracted_files-3/en/North_American_English_W/\"\n",
        "    converted_dir_A = f\"/content/drive/MyDrive/data/extracted_files-3/en/vae_converted_epoch_{epoch}\"\n",
        "\n",
        "    print(f\"Evaluating VAE performance after Epoch {epoch+1}\")\n",
        "    evaluate_metrics(original_dir=original_dir_A, converted_dir=converted_dir_A, mapping=mapping_A)\n",
        "\n",
        "# Function to Compute MCD (Placeholder)\n",
        "def compute_mcd(orig_audio, conv_audio):\n",
        "    return np.random.random()\n",
        "\n",
        "# Evaluation function for Mean Mel-Cepstral Distortion (MCD)\n",
        "def evaluate_metrics(original_dir, converted_dir, mapping):\n",
        "    mcd_scores = []\n",
        "\n",
        "    for orig_file, conv_file in mapping.items():\n",
        "        orig_path = os.path.join(original_dir, orig_file)\n",
        "        conv_path = os.path.join(converted_dir, conv_file)\n",
        "\n",
        "        if not os.path.exists(orig_path):\n",
        "            print(f\"Original file not found: {orig_path}\")\n",
        "            continue\n",
        "        if not os.path.exists(conv_path):\n",
        "            print(f\"Converted file not found: {conv_path}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            orig_audio, _ = torchaudio.load(orig_path)\n",
        "            conv_audio, _ = torchaudio.load(conv_path)\n",
        "        except RuntimeError as e:\n",
        "            print(f\"Error loading audio files. Original: {orig_path}, Converted: {conv_path}, Error: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Compute MCD\n",
        "        mcd_score = compute_mcd(orig_audio, conv_audio)\n",
        "        mcd_scores.append(mcd_score)\n",
        "\n",
        "    if mcd_scores:\n",
        "        mean_mcd = np.mean(mcd_scores)\n",
        "        print(f\"Mean MCD: {mean_mcd:.4f}\")\n",
        "    else:\n",
        "        print(\"No valid scores computed due to missing or corrupt files.\")\n",
        "\n",
        "# Start Training for VAE\n",
        "train_vae(dataloader_A)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "navjB5xwLIZn",
        "outputId": "17fc99aa-0a75-4c5e-c77f-891f4745ffe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Batch [1], Loss: 3.8443\n",
            "Epoch [1/5], Batch [2], Loss: 4.3802\n",
            "Epoch [1/5], Batch [3], Loss: 3.7984\n",
            "Epoch [1/5], Batch [4], Loss: 5.8977\n",
            "Epoch [1/5], Batch [5], Loss: 5.2885\n",
            "Epoch [1/5], Batch [6], Loss: 3.9073\n",
            "Epoch [1/5], Batch [7], Loss: 4.6379\n",
            "Epoch [1/5], Batch [8], Loss: 4.4580\n",
            "Epoch [1/5], Batch [9], Loss: 6.3473\n",
            "Epoch [1/5], Batch [10], Loss: 5.7203\n",
            "Epoch [1/5], Batch [11], Loss: 7.3422\n",
            "Epoch [1/5], Batch [12], Loss: 11.7899\n",
            "Epoch [1/5], Batch [13], Loss: 7.2433\n",
            "Epoch [1/5], Batch [14], Loss: 15.6629\n",
            "Epoch [1/5], Batch [15], Loss: 6.5123\n",
            "Epoch [1/5], Batch [16], Loss: 10.6417\n",
            "Epoch [1/5], Batch [17], Loss: 15.6176\n",
            "Epoch [1/5], Batch [18], Loss: 13.7358\n",
            "Epoch [1/5], Batch [19], Loss: 4.5846\n",
            "Epoch [1/5], Batch [20], Loss: 10.1587\n",
            "Epoch [1/5], Batch [21], Loss: 47.5850\n",
            "Epoch [1/5], Batch [22], Loss: 7.5190\n",
            "Epoch [1/5], Batch [23], Loss: 7.6221\n",
            "Epoch [1/5], Batch [24], Loss: 6.9065\n",
            "Epoch [1/5], Batch [25], Loss: 9.3862\n",
            "Epoch [1/5], Batch [26], Loss: 20.9936\n",
            "Epoch [1/5], Batch [27], Loss: 12.3257\n",
            "Epoch [1/5], Batch [28], Loss: 16.2381\n",
            "Epoch [1/5], Batch [29], Loss: 12.9351\n",
            "Epoch [1/5], Batch [30], Loss: 8.8039\n",
            "Epoch [1/5], Batch [31], Loss: 18.1890\n",
            "Epoch [1/5], Batch [32], Loss: 29.5739\n",
            "Epoch [1/5], Batch [33], Loss: 138.4238\n",
            "Epoch [1/5], Batch [34], Loss: 15.0036\n",
            "Epoch [1/5], Batch [35], Loss: 20.7896\n",
            "Epoch [1/5], Batch [36], Loss: 6.5750\n",
            "Epoch [1/5], Batch [37], Loss: 103.9823\n",
            "Epoch [1/5], Batch [38], Loss: 27.3085\n",
            "Epoch [1/5], Batch [39], Loss: 24.3411\n",
            "Epoch [1/5], Batch [40], Loss: 20.9136\n",
            "Epoch [1/5], Batch [41], Loss: 33.6205\n",
            "Epoch [1/5], Batch [42], Loss: 94.6696\n",
            "Epoch [1/5], Batch [43], Loss: 10.3047\n",
            "Epoch [1/5], Batch [44], Loss: 24.8703\n",
            "Epoch [1/5], Batch [45], Loss: 22.2782\n",
            "Epoch [1/5], Batch [46], Loss: 12.2873\n",
            "Epoch [1/5], Batch [47], Loss: 37.5773\n",
            "Epoch [1/5], Batch [48], Loss: 59.4792\n",
            "Epoch [1/5], Batch [49], Loss: 142.3258\n",
            "Epoch [1/5], Batch [50], Loss: 71.1420\n",
            "Epoch [1/5], Batch [51], Loss: 32.2678\n",
            "Epoch [1/5], Batch [52], Loss: 29.9759\n",
            "Epoch [1/5], Batch [53], Loss: 19.2594\n",
            "Epoch [1/5], Batch [54], Loss: 15.7740\n",
            "Epoch [1/5], Batch [55], Loss: 32.9555\n",
            "Epoch [1/5], Batch [56], Loss: 26.5150\n",
            "Epoch [1/5], Batch [57], Loss: 28.3060\n",
            "Epoch [1/5], Batch [58], Loss: 37.2542\n",
            "Epoch [1/5], Batch [59], Loss: 32.3334\n",
            "Epoch [1/5], Batch [60], Loss: 32.7589\n",
            "Epoch [1/5], Batch [61], Loss: 92.7897\n",
            "Epoch [1/5], Batch [62], Loss: 62.0173\n",
            "Epoch [1/5], Batch [63], Loss: 200.6987\n",
            "Epoch [1/5], Batch [64], Loss: 29.3530\n",
            "Epoch [1/5], Batch [65], Loss: 29.9284\n",
            "Epoch [1/5], Batch [66], Loss: 6.4642\n",
            "Epoch [1/5], Batch [67], Loss: 94.9162\n",
            "Epoch [1/5], Batch [68], Loss: 97.5844\n",
            "Epoch [1/5], Batch [69], Loss: 47.2271\n",
            "Epoch [1/5], Batch [70], Loss: 73.5408\n",
            "Epoch [1/5], Batch [71], Loss: 6124.9546\n",
            "Epoch [1/5], Batch [72], Loss: 82.8986\n",
            "Epoch [1/5], Batch [73], Loss: 49.7867\n",
            "Epoch [1/5], Batch [74], Loss: 35.1127\n",
            "Epoch [1/5], Batch [75], Loss: 19.4447\n",
            "Epoch [1/5], Batch [76], Loss: 507.1682\n",
            "Epoch [1/5], Batch [77], Loss: 66.9320\n",
            "Epoch [1/5], Batch [78], Loss: 132.5443\n",
            "Epoch [1/5], Batch [79], Loss: 41.0920\n",
            "Epoch [1/5], Batch [80], Loss: 53.3673\n",
            "Epoch [1/5], Batch [81], Loss: 34.4211\n",
            "Epoch [1/5], Batch [82], Loss: 109.3894\n",
            "Epoch [1/5], Batch [83], Loss: 26.3342\n",
            "Epoch [1/5], Batch [84], Loss: 1808.9703\n",
            "Epoch [1/5], Batch [85], Loss: 48.0733\n",
            "Epoch [1/5], Batch [86], Loss: 14.7094\n",
            "Epoch [1/5], Batch [87], Loss: 40.5552\n",
            "Epoch [1/5], Batch [88], Loss: 132.8472\n",
            "Epoch [1/5], Batch [89], Loss: 32.6015\n",
            "Epoch [1/5], Batch [90], Loss: 117.2520\n",
            "Epoch [1/5], Batch [91], Loss: 33.5457\n",
            "Epoch [1/5], Batch [92], Loss: 20.8963\n",
            "Epoch [1/5], Batch [93], Loss: 72.7113\n",
            "Epoch [1/5], Batch [94], Loss: 64.6479\n",
            "Epoch [1/5], Batch [95], Loss: 70.2852\n",
            "Epoch [1/5], Batch [96], Loss: 65.0792\n",
            "Epoch [1/5], Batch [97], Loss: 76.5178\n",
            "Epoch [1/5], Batch [98], Loss: 70.7621\n",
            "Epoch [1/5], Batch [99], Loss: 47.1180\n",
            "Epoch [1/5], Batch [100], Loss: 18.2153\n",
            "Epoch [1/5] completed.\n",
            "Evaluating VAE performance after Epoch 1\n",
            "Mean MCD: 0.5319\n",
            "Epoch [2/5], Batch [1], Loss: 57.6426\n",
            "Epoch [2/5], Batch [2], Loss: 46.9613\n",
            "Epoch [2/5], Batch [3], Loss: 28.9891\n",
            "Epoch [2/5], Batch [4], Loss: 11.6683\n",
            "Epoch [2/5], Batch [5], Loss: 129.8549\n",
            "Epoch [2/5], Batch [6], Loss: 75.7947\n",
            "Epoch [2/5], Batch [7], Loss: 63.1946\n",
            "Epoch [2/5], Batch [8], Loss: 47.2150\n",
            "Epoch [2/5], Batch [9], Loss: 64.3034\n",
            "Epoch [2/5], Batch [10], Loss: 107.8505\n",
            "Epoch [2/5], Batch [11], Loss: 43.3095\n",
            "Epoch [2/5], Batch [12], Loss: 46.3093\n",
            "Epoch [2/5], Batch [13], Loss: 62.2690\n",
            "Epoch [2/5], Batch [14], Loss: 106.3878\n",
            "Epoch [2/5], Batch [15], Loss: 35.9899\n",
            "Epoch [2/5], Batch [16], Loss: 51.7318\n",
            "Epoch [2/5], Batch [17], Loss: 29.7462\n",
            "Epoch [2/5], Batch [18], Loss: 40.5095\n",
            "Epoch [2/5], Batch [19], Loss: 17.5026\n",
            "Epoch [2/5], Batch [20], Loss: 66.4043\n",
            "Epoch [2/5], Batch [21], Loss: 742712.8125\n",
            "Epoch [2/5], Batch [22], Loss: 38.6900\n",
            "Epoch [2/5], Batch [23], Loss: 21.6999\n",
            "Epoch [2/5], Batch [24], Loss: 29.0106\n",
            "Epoch [2/5], Batch [25], Loss: 78.2540\n",
            "Epoch [2/5], Batch [26], Loss: 19.3935\n",
            "Epoch [2/5], Batch [27], Loss: 7251.7358\n",
            "Epoch [2/5], Batch [28], Loss: 46.8363\n",
            "Epoch [2/5], Batch [29], Loss: 29.2866\n",
            "Epoch [2/5], Batch [30], Loss: 6991.2959\n",
            "Epoch [2/5], Batch [31], Loss: 64.8830\n",
            "Epoch [2/5], Batch [32], Loss: 71.5707\n",
            "Epoch [2/5], Batch [33], Loss: 43.9411\n",
            "Epoch [2/5], Batch [34], Loss: 26.3188\n",
            "Epoch [2/5], Batch [35], Loss: 52.6357\n",
            "Epoch [2/5], Batch [36], Loss: 65.7664\n",
            "Epoch [2/5], Batch [37], Loss: 97.5598\n",
            "Epoch [2/5], Batch [38], Loss: 80.0206\n",
            "Epoch [2/5], Batch [39], Loss: 1420.6044\n",
            "Epoch [2/5], Batch [40], Loss: 57.8099\n",
            "Epoch [2/5], Batch [41], Loss: 53.5511\n",
            "Epoch [2/5], Batch [42], Loss: 67.9986\n",
            "Epoch [2/5], Batch [43], Loss: 13.0351\n",
            "Epoch [2/5], Batch [44], Loss: 68.4570\n",
            "Epoch [2/5], Batch [45], Loss: 15.4268\n",
            "Epoch [2/5], Batch [46], Loss: 28.4650\n",
            "Epoch [2/5], Batch [47], Loss: 50.5912\n",
            "Epoch [2/5], Batch [48], Loss: 29.2484\n",
            "Epoch [2/5], Batch [49], Loss: 257.6676\n",
            "Epoch [2/5], Batch [50], Loss: 94.7412\n",
            "Epoch [2/5], Batch [51], Loss: 22.6630\n",
            "Epoch [2/5], Batch [52], Loss: 49.4415\n",
            "Epoch [2/5], Batch [53], Loss: 106.7142\n",
            "Epoch [2/5], Batch [54], Loss: 59.8264\n",
            "Epoch [2/5], Batch [55], Loss: 50.9466\n",
            "Epoch [2/5], Batch [56], Loss: 47.5771\n",
            "Epoch [2/5], Batch [57], Loss: 33.4064\n",
            "Epoch [2/5], Batch [58], Loss: 47.2366\n",
            "Epoch [2/5], Batch [59], Loss: 37.9920\n",
            "Epoch [2/5], Batch [60], Loss: 47.8798\n",
            "Epoch [2/5], Batch [61], Loss: 71.1286\n",
            "Epoch [2/5], Batch [62], Loss: 76.7799\n",
            "Epoch [2/5], Batch [63], Loss: 4570.0396\n",
            "Epoch [2/5], Batch [64], Loss: 26.5924\n",
            "Epoch [2/5], Batch [65], Loss: 41.0673\n",
            "Epoch [2/5], Batch [66], Loss: 63.8878\n",
            "Epoch [2/5], Batch [67], Loss: 32.6634\n",
            "Epoch [2/5], Batch [68], Loss: 572.8326\n",
            "Epoch [2/5], Batch [69], Loss: 129.6247\n",
            "Epoch [2/5], Batch [70], Loss: 75.2612\n",
            "Epoch [2/5], Batch [71], Loss: 24.5601\n",
            "Epoch [2/5], Batch [72], Loss: 141.5007\n",
            "Epoch [2/5], Batch [73], Loss: 24.9801\n",
            "Epoch [2/5], Batch [74], Loss: 67.1355\n",
            "Epoch [2/5], Batch [75], Loss: 27.4819\n",
            "Epoch [2/5], Batch [76], Loss: 56.1654\n",
            "Epoch [2/5], Batch [77], Loss: 49.6520\n",
            "Epoch [2/5], Batch [78], Loss: 25.0960\n",
            "Epoch [2/5], Batch [79], Loss: 19.8702\n",
            "Epoch [2/5], Batch [80], Loss: 136.4128\n",
            "Epoch [2/5], Batch [81], Loss: 59.4081\n",
            "Epoch [2/5], Batch [82], Loss: 123.4821\n",
            "Epoch [2/5], Batch [83], Loss: 154.2882\n",
            "Epoch [2/5], Batch [84], Loss: 27.6503\n",
            "Epoch [2/5], Batch [85], Loss: 17.4688\n",
            "Epoch [2/5], Batch [86], Loss: 60.3905\n",
            "Epoch [2/5], Batch [87], Loss: 54.6463\n",
            "Epoch [2/5], Batch [88], Loss: 413.1260\n",
            "Epoch [2/5], Batch [89], Loss: 58.1617\n",
            "Epoch [2/5], Batch [90], Loss: 31.3477\n",
            "Epoch [2/5], Batch [91], Loss: 8899294.0000\n",
            "Epoch [2/5], Batch [92], Loss: 20.6818\n",
            "Epoch [2/5], Batch [93], Loss: 93.1360\n",
            "Epoch [2/5], Batch [94], Loss: 54.7254\n",
            "Epoch [2/5], Batch [95], Loss: 50.4067\n",
            "Epoch [2/5], Batch [96], Loss: 42.6770\n",
            "Epoch [2/5], Batch [97], Loss: 8530.8223\n",
            "Epoch [2/5], Batch [98], Loss: 84.3990\n",
            "Epoch [2/5], Batch [99], Loss: 15.0693\n",
            "Epoch [2/5], Batch [100], Loss: 40.4172\n",
            "Epoch [2/5] completed.\n",
            "Evaluating VAE performance after Epoch 2\n",
            "Mean MCD: 0.5158\n",
            "Epoch [3/5], Batch [1], Loss: 94323.5391\n",
            "Epoch [3/5], Batch [2], Loss: 26.0661\n",
            "Epoch [3/5], Batch [3], Loss: 58.9418\n",
            "Epoch [3/5], Batch [4], Loss: 95938152.0000\n",
            "Epoch [3/5], Batch [5], Loss: 33.1539\n",
            "Epoch [3/5], Batch [6], Loss: 1915268480.0000\n",
            "Epoch [3/5], Batch [7], Loss: 36.1248\n",
            "Epoch [3/5], Batch [8], Loss: 219.3029\n",
            "Epoch [3/5], Batch [9], Loss: 36.1002\n",
            "Epoch [3/5], Batch [10], Loss: 12.4377\n",
            "Epoch [3/5], Batch [11], Loss: 138.1789\n",
            "Epoch [3/5], Batch [12], Loss: 18.4403\n",
            "Epoch [3/5], Batch [13], Loss: 53.7027\n",
            "Epoch [3/5], Batch [14], Loss: 397.5796\n",
            "Epoch [3/5], Batch [15], Loss: 36007.2305\n",
            "Epoch [3/5], Batch [16], Loss: 120.8274\n",
            "Epoch [3/5], Batch [17], Loss: 1509.8258\n",
            "Epoch [3/5], Batch [18], Loss: 90.4893\n",
            "Epoch [3/5], Batch [19], Loss: 2233.9094\n",
            "Epoch [3/5], Batch [20], Loss: 117.2438\n",
            "Epoch [3/5], Batch [21], Loss: 53.3933\n",
            "Epoch [3/5], Batch [22], Loss: 39.4935\n",
            "Epoch [3/5], Batch [23], Loss: 60.4069\n",
            "Epoch [3/5], Batch [24], Loss: 80256.7891\n",
            "Epoch [3/5], Batch [25], Loss: 79.1404\n",
            "Epoch [3/5], Batch [26], Loss: 62.1823\n",
            "Epoch [3/5], Batch [27], Loss: 147.7078\n",
            "Epoch [3/5], Batch [28], Loss: 37.6213\n",
            "Epoch [3/5], Batch [29], Loss: 133.8008\n",
            "Epoch [3/5], Batch [30], Loss: 105.0810\n",
            "Epoch [3/5], Batch [31], Loss: 68.9971\n",
            "Epoch [3/5], Batch [32], Loss: 21.7836\n",
            "Epoch [3/5], Batch [33], Loss: 1777.7053\n",
            "Epoch [3/5], Batch [34], Loss: 88.0001\n",
            "Epoch [3/5], Batch [35], Loss: 47.1716\n",
            "Epoch [3/5], Batch [36], Loss: 72.0052\n",
            "Epoch [3/5], Batch [37], Loss: 9.8201\n",
            "Epoch [3/5], Batch [38], Loss: 52.4811\n",
            "Epoch [3/5], Batch [39], Loss: 39.5402\n",
            "Epoch [3/5], Batch [40], Loss: 157.6829\n",
            "Epoch [3/5], Batch [41], Loss: 2313486848.0000\n",
            "Epoch [3/5], Batch [42], Loss: 112.9183\n",
            "Epoch [3/5], Batch [43], Loss: 51.5313\n",
            "Epoch [3/5], Batch [44], Loss: 41.8894\n",
            "Epoch [3/5], Batch [45], Loss: 62.1180\n",
            "Epoch [3/5], Batch [46], Loss: 30.1750\n",
            "Epoch [3/5], Batch [47], Loss: 87.6589\n",
            "Epoch [3/5], Batch [48], Loss: 30.4693\n",
            "Epoch [3/5], Batch [49], Loss: 61.2117\n",
            "Epoch [3/5], Batch [50], Loss: 42.6531\n",
            "Epoch [3/5], Batch [51], Loss: 101.5902\n",
            "Epoch [3/5], Batch [52], Loss: 45.6118\n",
            "Epoch [3/5], Batch [53], Loss: 86.7974\n",
            "Epoch [3/5], Batch [54], Loss: 24.5625\n",
            "Epoch [3/5], Batch [55], Loss: 48.6222\n",
            "Epoch [3/5], Batch [56], Loss: 28.1193\n",
            "Epoch [3/5], Batch [57], Loss: 209.1485\n",
            "Epoch [3/5], Batch [58], Loss: 137.9803\n",
            "Epoch [3/5], Batch [59], Loss: 24.5261\n",
            "Epoch [3/5], Batch [60], Loss: 33.6526\n",
            "Epoch [3/5], Batch [61], Loss: 53.7798\n",
            "Epoch [3/5], Batch [62], Loss: 59.6504\n",
            "Epoch [3/5], Batch [63], Loss: 6506.6426\n",
            "Epoch [3/5], Batch [64], Loss: 76.6367\n",
            "Epoch [3/5], Batch [65], Loss: 103.7175\n",
            "Epoch [3/5], Batch [66], Loss: 51.8877\n",
            "Epoch [3/5], Batch [67], Loss: 125.4783\n",
            "Epoch [3/5], Batch [68], Loss: 74.4815\n",
            "Epoch [3/5], Batch [69], Loss: 106.7540\n",
            "Epoch [3/5], Batch [70], Loss: 71.0363\n",
            "Epoch [3/5], Batch [71], Loss: 33.7371\n",
            "Epoch [3/5], Batch [72], Loss: 178.4825\n",
            "Epoch [3/5], Batch [73], Loss: 87.9869\n",
            "Epoch [3/5], Batch [74], Loss: 30100.3691\n",
            "Epoch [3/5], Batch [75], Loss: 57.8074\n",
            "Epoch [3/5], Batch [76], Loss: 32.5872\n",
            "Epoch [3/5], Batch [77], Loss: 141.0677\n",
            "Epoch [3/5], Batch [78], Loss: 150.6386\n",
            "Epoch [3/5], Batch [79], Loss: 62.7467\n",
            "Epoch [3/5], Batch [80], Loss: 127.3924\n",
            "Epoch [3/5], Batch [81], Loss: 29785.3477\n",
            "Epoch [3/5], Batch [82], Loss: 88.7694\n",
            "Epoch [3/5], Batch [83], Loss: 65.8992\n",
            "Epoch [3/5], Batch [84], Loss: 49.8001\n",
            "Epoch [3/5], Batch [85], Loss: 33.6181\n",
            "Epoch [3/5], Batch [86], Loss: 19.7051\n",
            "Epoch [3/5], Batch [87], Loss: 49.9573\n",
            "Epoch [3/5], Batch [88], Loss: 44.4439\n",
            "Epoch [3/5], Batch [89], Loss: 51.3497\n",
            "Epoch [3/5], Batch [90], Loss: 235.3297\n",
            "Epoch [3/5], Batch [91], Loss: 119.6556\n",
            "Epoch [3/5], Batch [92], Loss: 106.1059\n",
            "Epoch [3/5], Batch [93], Loss: 59.3878\n",
            "Epoch [3/5], Batch [94], Loss: 93.3114\n",
            "Epoch [3/5], Batch [95], Loss: 20.8245\n",
            "Epoch [3/5], Batch [96], Loss: 48.7653\n",
            "Epoch [3/5], Batch [97], Loss: 56.2508\n",
            "Epoch [3/5], Batch [98], Loss: 185.1999\n",
            "Epoch [3/5], Batch [99], Loss: 38.4508\n",
            "Epoch [3/5], Batch [100], Loss: 4050716.2500\n",
            "Epoch [3/5] completed.\n",
            "Evaluating VAE performance after Epoch 3\n",
            "Mean MCD: 0.5239\n",
            "Epoch [4/5], Batch [1], Loss: 144.4853\n",
            "Epoch [4/5], Batch [2], Loss: 427.5716\n",
            "Epoch [4/5], Batch [3], Loss: 32.7274\n",
            "Epoch [4/5], Batch [4], Loss: 59.3619\n",
            "Epoch [4/5], Batch [5], Loss: 88.9482\n",
            "Epoch [4/5], Batch [6], Loss: 72.3550\n",
            "Epoch [4/5], Batch [7], Loss: 40.8421\n",
            "Epoch [4/5], Batch [8], Loss: 34.3949\n",
            "Epoch [4/5], Batch [9], Loss: 67.0506\n",
            "Epoch [4/5], Batch [10], Loss: 91.0977\n",
            "Epoch [4/5], Batch [11], Loss: 33.7104\n",
            "Epoch [4/5], Batch [12], Loss: 46.2407\n",
            "Epoch [4/5], Batch [13], Loss: 72.6971\n",
            "Epoch [4/5], Batch [14], Loss: 20.6847\n",
            "Epoch [4/5], Batch [15], Loss: 113.6239\n",
            "Epoch [4/5], Batch [16], Loss: 8.3681\n",
            "Epoch [4/5], Batch [17], Loss: 45.0464\n",
            "Epoch [4/5], Batch [18], Loss: 54.0834\n",
            "Epoch [4/5], Batch [19], Loss: 59.0995\n",
            "Epoch [4/5], Batch [20], Loss: 29.3504\n",
            "Epoch [4/5], Batch [21], Loss: 188.6129\n",
            "Epoch [4/5], Batch [22], Loss: 20.3585\n",
            "Epoch [4/5], Batch [23], Loss: 47.2062\n",
            "Epoch [4/5], Batch [24], Loss: 28.7592\n",
            "Epoch [4/5], Batch [25], Loss: 61.7723\n",
            "Epoch [4/5], Batch [26], Loss: 50.0670\n",
            "Epoch [4/5], Batch [27], Loss: 32.4335\n",
            "Epoch [4/5], Batch [28], Loss: 50.8532\n",
            "Epoch [4/5], Batch [29], Loss: 134.8561\n",
            "Epoch [4/5], Batch [30], Loss: 32.1118\n",
            "Epoch [4/5], Batch [31], Loss: 14.4744\n",
            "Epoch [4/5], Batch [32], Loss: 33.6309\n",
            "Epoch [4/5], Batch [33], Loss: 95.4111\n",
            "Epoch [4/5], Batch [34], Loss: 49.8374\n",
            "Epoch [4/5], Batch [35], Loss: 618.6962\n",
            "Epoch [4/5], Batch [36], Loss: 24.3639\n",
            "Epoch [4/5], Batch [37], Loss: 30.5629\n",
            "Epoch [4/5], Batch [38], Loss: 58.7997\n",
            "Epoch [4/5], Batch [39], Loss: 86.7274\n",
            "Epoch [4/5], Batch [40], Loss: 63.6799\n",
            "Epoch [4/5], Batch [41], Loss: 45.6918\n",
            "Epoch [4/5], Batch [42], Loss: 35.7234\n",
            "Epoch [4/5], Batch [43], Loss: 37.8578\n",
            "Epoch [4/5], Batch [44], Loss: 21.4327\n",
            "Epoch [4/5], Batch [45], Loss: 9.2492\n",
            "Epoch [4/5], Batch [46], Loss: 35.5025\n",
            "Epoch [4/5], Batch [47], Loss: 49.2648\n",
            "Epoch [4/5], Batch [48], Loss: 24.6094\n",
            "Epoch [4/5], Batch [49], Loss: 156.4326\n",
            "Epoch [4/5], Batch [50], Loss: 21.6743\n",
            "Epoch [4/5], Batch [51], Loss: 55.4444\n",
            "Epoch [4/5], Batch [52], Loss: 69.1129\n",
            "Epoch [4/5], Batch [53], Loss: 53.9970\n",
            "Epoch [4/5], Batch [54], Loss: 121.2915\n",
            "Epoch [4/5], Batch [55], Loss: 105.5779\n",
            "Epoch [4/5], Batch [56], Loss: 38.1874\n",
            "Epoch [4/5], Batch [57], Loss: 25.9974\n",
            "Epoch [4/5], Batch [58], Loss: 53.2661\n",
            "Epoch [4/5], Batch [59], Loss: 131.8007\n",
            "Epoch [4/5], Batch [60], Loss: 78.5729\n",
            "Epoch [4/5], Batch [61], Loss: 138.5959\n",
            "Epoch [4/5], Batch [62], Loss: 142.0598\n",
            "Epoch [4/5], Batch [63], Loss: 109.6522\n",
            "Epoch [4/5], Batch [64], Loss: 92.4514\n",
            "Epoch [4/5], Batch [65], Loss: 68.8533\n",
            "Epoch [4/5], Batch [66], Loss: 113.1765\n",
            "Epoch [4/5], Batch [67], Loss: 95.5182\n",
            "Epoch [4/5], Batch [68], Loss: 114552.2969\n",
            "Epoch [4/5], Batch [69], Loss: 652.6398\n",
            "Epoch [4/5], Batch [70], Loss: 29.5429\n",
            "Epoch [4/5], Batch [71], Loss: 86.2555\n",
            "Epoch [4/5], Batch [72], Loss: 34.7368\n",
            "Epoch [4/5], Batch [73], Loss: 55.4928\n",
            "Epoch [4/5], Batch [74], Loss: 33052.5664\n",
            "Epoch [4/5], Batch [75], Loss: 34.1942\n",
            "Epoch [4/5], Batch [76], Loss: 37.0836\n",
            "Epoch [4/5], Batch [77], Loss: 116.1950\n",
            "Epoch [4/5], Batch [78], Loss: 199.5377\n",
            "Epoch [4/5], Batch [79], Loss: 69.7241\n",
            "Epoch [4/5], Batch [80], Loss: 96.2113\n",
            "Epoch [4/5], Batch [81], Loss: 76.4749\n",
            "Epoch [4/5], Batch [82], Loss: 85.2374\n",
            "Epoch [4/5], Batch [83], Loss: 71.4796\n",
            "Epoch [4/5], Batch [84], Loss: 63.1390\n",
            "Epoch [4/5], Batch [85], Loss: 364.4872\n",
            "Epoch [4/5], Batch [86], Loss: 60.3504\n",
            "Epoch [4/5], Batch [87], Loss: 28.1328\n",
            "Epoch [4/5], Batch [88], Loss: 73.2501\n",
            "Epoch [4/5], Batch [89], Loss: 117.9286\n",
            "Epoch [4/5], Batch [90], Loss: 133.2240\n",
            "Epoch [4/5], Batch [91], Loss: 64.0397\n",
            "Epoch [4/5], Batch [92], Loss: 17.0829\n",
            "Epoch [4/5], Batch [93], Loss: 52.9282\n",
            "Epoch [4/5], Batch [94], Loss: 29.8398\n",
            "Epoch [4/5], Batch [95], Loss: 70.6184\n",
            "Epoch [4/5], Batch [96], Loss: 27.7838\n",
            "Epoch [4/5], Batch [97], Loss: 55.7308\n",
            "Epoch [4/5], Batch [98], Loss: 90.2374\n",
            "Epoch [4/5], Batch [99], Loss: 83.4082\n",
            "Epoch [4/5], Batch [100], Loss: 46.3913\n",
            "Epoch [4/5] completed.\n",
            "Evaluating VAE performance after Epoch 4\n",
            "Mean MCD: 0.4391\n",
            "Epoch [5/5], Batch [1], Loss: 47.2350\n",
            "Epoch [5/5], Batch [2], Loss: 82.9581\n",
            "Epoch [5/5], Batch [3], Loss: 35.2486\n",
            "Epoch [5/5], Batch [4], Loss: 157.2771\n",
            "Epoch [5/5], Batch [5], Loss: 54.9675\n",
            "Epoch [5/5], Batch [6], Loss: 81.9971\n",
            "Epoch [5/5], Batch [7], Loss: 104.6095\n",
            "Epoch [5/5], Batch [8], Loss: 68.6457\n",
            "Epoch [5/5], Batch [9], Loss: 54.9813\n",
            "Epoch [5/5], Batch [10], Loss: 92.2637\n",
            "Epoch [5/5], Batch [11], Loss: 11.5341\n",
            "Epoch [5/5], Batch [12], Loss: 85.9531\n",
            "Epoch [5/5], Batch [13], Loss: 99.2235\n",
            "Epoch [5/5], Batch [14], Loss: 82.9303\n",
            "Epoch [5/5], Batch [15], Loss: 28.2882\n",
            "Epoch [5/5], Batch [16], Loss: 247.5498\n",
            "Epoch [5/5], Batch [17], Loss: 46.1998\n",
            "Epoch [5/5], Batch [18], Loss: 44.7982\n",
            "Epoch [5/5], Batch [19], Loss: 44.9368\n",
            "Epoch [5/5], Batch [20], Loss: 37.6811\n",
            "Epoch [5/5], Batch [21], Loss: 76.1145\n",
            "Epoch [5/5], Batch [22], Loss: 27.2921\n",
            "Epoch [5/5], Batch [23], Loss: 113.2293\n",
            "Epoch [5/5], Batch [24], Loss: 63.9912\n",
            "Epoch [5/5], Batch [25], Loss: 37.9411\n",
            "Epoch [5/5], Batch [26], Loss: 134.3168\n",
            "Epoch [5/5], Batch [27], Loss: 17.3478\n",
            "Epoch [5/5], Batch [28], Loss: 63.2833\n",
            "Epoch [5/5], Batch [29], Loss: 45.5739\n",
            "Epoch [5/5], Batch [30], Loss: 53.4665\n",
            "Epoch [5/5], Batch [31], Loss: 99.3923\n",
            "Epoch [5/5], Batch [32], Loss: 39.7196\n",
            "Epoch [5/5], Batch [33], Loss: 70.7655\n",
            "Epoch [5/5], Batch [34], Loss: 59.9272\n",
            "Epoch [5/5], Batch [35], Loss: 70.0770\n",
            "Epoch [5/5], Batch [36], Loss: 21.2958\n",
            "Epoch [5/5], Batch [37], Loss: 60.5587\n",
            "Epoch [5/5], Batch [38], Loss: 32.7111\n",
            "Epoch [5/5], Batch [39], Loss: 3044.5308\n",
            "Epoch [5/5], Batch [40], Loss: 86.8340\n",
            "Epoch [5/5], Batch [41], Loss: 64.2466\n",
            "Epoch [5/5], Batch [42], Loss: 9.5629\n",
            "Epoch [5/5], Batch [43], Loss: 58.6608\n",
            "Epoch [5/5], Batch [44], Loss: 115.1104\n",
            "Epoch [5/5], Batch [45], Loss: 33.7756\n",
            "Epoch [5/5], Batch [46], Loss: 21.8387\n",
            "Epoch [5/5], Batch [47], Loss: 62.3227\n",
            "Epoch [5/5], Batch [48], Loss: 39.8827\n",
            "Epoch [5/5], Batch [49], Loss: 29.3060\n",
            "Epoch [5/5], Batch [50], Loss: 94.3967\n",
            "Epoch [5/5], Batch [51], Loss: 38.8560\n",
            "Epoch [5/5], Batch [52], Loss: 247335.6562\n",
            "Epoch [5/5], Batch [53], Loss: 49.9470\n",
            "Epoch [5/5], Batch [54], Loss: 39.3401\n",
            "Epoch [5/5], Batch [55], Loss: 46.1408\n",
            "Epoch [5/5], Batch [56], Loss: 50.9273\n",
            "Epoch [5/5], Batch [57], Loss: 14.6441\n",
            "Epoch [5/5], Batch [58], Loss: 43.1806\n",
            "Epoch [5/5], Batch [59], Loss: 28.6177\n",
            "Epoch [5/5], Batch [60], Loss: 40.6754\n",
            "Epoch [5/5], Batch [61], Loss: 54.6284\n",
            "Epoch [5/5], Batch [62], Loss: 40.9550\n",
            "Epoch [5/5], Batch [63], Loss: 100.4376\n",
            "Epoch [5/5], Batch [64], Loss: 113.8090\n",
            "Epoch [5/5], Batch [65], Loss: 123.3914\n",
            "Epoch [5/5], Batch [66], Loss: 39.0986\n",
            "Epoch [5/5], Batch [67], Loss: 53.2447\n",
            "Epoch [5/5], Batch [68], Loss: 144.7686\n",
            "Epoch [5/5], Batch [69], Loss: 46.3388\n",
            "Epoch [5/5], Batch [70], Loss: 21.3703\n",
            "Epoch [5/5], Batch [71], Loss: 39.7273\n",
            "Epoch [5/5], Batch [72], Loss: 37.9542\n",
            "Epoch [5/5], Batch [73], Loss: 17.4793\n",
            "Epoch [5/5], Batch [74], Loss: 15.3199\n",
            "Epoch [5/5], Batch [75], Loss: 43.2410\n",
            "Epoch [5/5], Batch [76], Loss: 211.1254\n",
            "Epoch [5/5], Batch [77], Loss: 40.4251\n",
            "Epoch [5/5], Batch [78], Loss: 29.6345\n",
            "Epoch [5/5], Batch [79], Loss: 223714320.0000\n",
            "Epoch [5/5], Batch [80], Loss: 9414.4512\n",
            "Epoch [5/5], Batch [81], Loss: 52.6756\n",
            "Epoch [5/5], Batch [82], Loss: 56.2659\n",
            "Epoch [5/5], Batch [83], Loss: 75.6122\n",
            "Epoch [5/5], Batch [84], Loss: 56.7644\n",
            "Epoch [5/5], Batch [85], Loss: 51.1041\n",
            "Epoch [5/5], Batch [86], Loss: 54.9630\n",
            "Epoch [5/5], Batch [87], Loss: 82.0220\n",
            "Epoch [5/5], Batch [88], Loss: 134.7966\n",
            "Epoch [5/5], Batch [89], Loss: 86.5069\n",
            "Epoch [5/5], Batch [90], Loss: 45.9215\n",
            "Epoch [5/5], Batch [91], Loss: 264.5848\n",
            "Epoch [5/5], Batch [92], Loss: 34.0040\n",
            "Epoch [5/5], Batch [93], Loss: 28.6453\n",
            "Epoch [5/5], Batch [94], Loss: 57.3823\n",
            "Epoch [5/5], Batch [95], Loss: 73.7343\n",
            "Epoch [5/5], Batch [96], Loss: 511.1360\n",
            "Epoch [5/5], Batch [97], Loss: 132.9798\n",
            "Epoch [5/5], Batch [98], Loss: 60.6896\n",
            "Epoch [5/5], Batch [99], Loss: 58.2445\n",
            "Epoch [5/5], Batch [100], Loss: 86.0237\n",
            "Epoch [5/5] completed.\n",
            "Evaluating VAE performance after Epoch 5\n",
            "Mean MCD: 0.5301\n",
            "Training completed successfully!\n"
          ]
        }
      ]
    }
  ]
}